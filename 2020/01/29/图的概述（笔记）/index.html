<!-- build time:Wed Jan 29 2020 02:48:09 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="yandex-verification" content="3ac9ae36ddebb425"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://lsj210001.github.io").hostname,root:"/",scheme:"Gemini",version:"7.7.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!0},path:"search.xml",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="本文为《深入浅出图神经网络：GNN原理解析》第一章的笔记图的概述图（Graph）是一个具有广泛含义的对象：在数学中，图是图论的主要研究对象；在计算机工程领域，图是一种常见的数据结构；在数据科学中，图被用来广泛描述各类关系型数据。通常，图被用来表示物体与物体之间的关系，例如：化学分子通信网络社交网络事实上，任何一个包含二元关系的系统都可以用图来描述。图的基本定义图可以表示为顶点（Vertex）和连接"><meta property="og:type" content="article"><meta property="og:title" content="图的概述（笔记）"><meta property="og:url" content="https://lsj210001.github.io/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/index.html"><meta property="og:site_name" content="Morgan’s Blog"><meta property="og:description" content="本文为《深入浅出图神经网络：GNN原理解析》第一章的笔记图的概述图（Graph）是一个具有广泛含义的对象：在数学中，图是图论的主要研究对象；在计算机工程领域，图是一种常见的数据结构；在数据科学中，图被用来广泛描述各类关系型数据。通常，图被用来表示物体与物体之间的关系，例如：化学分子通信网络社交网络事实上，任何一个包含二元关系的系统都可以用图来描述。图的基本定义图可以表示为顶点（Vertex）和连接"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200126234551.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127000108.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127110459.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127111035.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127115718.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128103152.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128105325.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128110458.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128111838.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128171828.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128172817.png"><meta property="og:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128220741.png"><meta property="article:published_time" content="2020-01-29T02:02:20.000Z"><meta property="article:modified_time" content="2020-01-28T18:39:20.576Z"><meta property="article:author" content="Morgan"><meta property="article:tag" content="图论"><meta property="article:tag" content="Algorithm"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200126234551.png"><link rel="canonical" href="https://lsj210001.github.io/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>图的概述（笔记） | Morgan’s Blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?7680854d4de4d9d556876810e04cf1c8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Morgan’s Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-bullhorn"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">6</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">16</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">4</span></a></li><li class="menu-item menu-item-docs"><a href="/docs/" rel="section"><i class="fa fa-fw fa-book"></i>Docs</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0" title="Fork NexT on GitHub" aria-label="Fork NexT on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://lsj210001.github.io/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/apple-touch-icon-next.png"><meta itemprop="name" content="Morgan"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Morgan’s Blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">图的概述（笔记）<span class="exturl post-edit-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmcvZWRpdC9zb3VyY2Uvc291cmNlL19wb3N0cy/lm77nmoTmpoLov7DvvIjnrJTorrDvvIkubWQ=" title="编辑"><i class="fa fa-pencil"></i></span></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-01-29 02:02:20" itemprop="dateCreated datePublished" datetime="2020-01-29T02:02:20Z">2020-01-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-28 18:39:20" itemprop="dateModified" datetime="2020-01-28T18:39:20Z">2020-01-28</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span> </a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus：</span> <a title="disqus" href="/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/29/图的概述（笔记）/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>8.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>本文为<span class="exturl" data-url="aHR0cHM6Ly9pdGVtLmpkLmNvbS8xMjYxNTA2NS5odG1s" title="https://item.jd.com/12615065.html">《深入浅出图神经网络：GNN原理解析》<i class="fa fa-external-link"></i></span>第一章的笔记</p></blockquote><h1 id="图的概述">图的概述</h1><p><strong>图</strong>（Graph）是一个具有广泛含义的对象：</p><ul><li>在数学中，图是图论的主要研究对象；</li><li>在计算机工程领域，图是一种常见的数据结构；</li><li>在数据科学中，图被用来广泛描述各类关系型数据。</li></ul><p>通常，图被用来表示<strong>物体与物体之间的关系</strong>，例如：</p><ul><li>化学分子</li><li>通信网络</li><li>社交网络</li></ul><p>事实上，任何一个包含<strong>二元关系</strong>的系统都可以用图来描述。</p><h1 id="图的基本定义">图的基本定义</h1><p>图可以表示为<strong>顶点</strong>（Vertex）和连接顶点的<strong>边</strong>（Edge）的集合，记为 <span class="math inline">\(G = ( V ,\space E )\)</span>，其中 <span class="math inline">\(V\)</span> 是顶点集合，<span class="math inline">\(E\)</span> 是边集合。</p><p>我们可设图 <span class="math inline">\(G\)</span> 的顶点数为 <span class="math inline">\(N\)</span>，边数为 <span class="math inline">\(M\)</span>，一条连接顶点 <span class="math inline">\(v _ { i } , \space v _ { j } \in V\)</span> 的边记为 <span class="math inline">\(\left( v _ { i } , \space v _ { j } \right)\)</span> 或者 <span class="math inline">\(e _ { i j }\)</span>。</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200126234551.png" alt="图G的定义"><figcaption>图G的定义</figcaption></figure><h2 id="图的基本类型">图的基本类型</h2><h3 id="有向图和无向图">有向图和无向图</h3><p>如果图中的边存在方向性，则称这样的边为有向边 <span class="math inline">\(e _ { ij } = &lt; v _ { i } , \space v _ { j } &gt;\)</span>，其中 <span class="math inline">\(v _ { i }\)</span> 是这条有向边的起点，<span class="math inline">\(v _ { j }\)</span> 是这条有向边的终点，包含有向边的图称为有向图。</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127000108.png" alt="有向图"><figcaption>有向图</figcaption></figure><p>与有向图相对应的是无向图，无向图中的边都是无向边，我们可以认为无向边是对称的，同时包含两个方向：<span class="math inline">\(e _ { ij } = &lt; v _ { i } , \space v _ { j } &gt; = &lt; v _ { j } , \space v _ { i } &gt; = e _ { ji }\)</span>。</p><h3 id="非加权图与加权图">非加权图与加权图</h3><p>如果图里的每条边都有一个实数与之对应，我们称这样的图为加权图，该实数称为对应边上的权重。</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127110459.png" alt="加权图"><figcaption>加权图</figcaption></figure><p>在实际场景中，权重可以代表两地之间的路程或运输成本。一般情况下，我们习惯把权重抽象成两个顶点之间的<strong>连接强度</strong>。与之相反的是非加权图，我们可以认为非加权图各边上的权重是一样的。</p><h3 id="连通图与非连通图">连通图与非连通图</h3><p>如果图中存在<strong>孤立的顶点</strong>，没有任何边与之相连，这样的图被称为非连通图。（下图中应该有一个孤立顶点 <span class="math inline">\(v _ { 5 }\)</span> 没有绘制出来）</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127111035.png" alt="非连通图"><figcaption>非连通图</figcaption></figure><p>图中不存在孤立顶点的图称为连通图。</p><h3 id="二部图">二部图</h3><p>二部图是一类特殊的图。我们将 <span class="math inline">\(G\)</span> 中的顶点集合 <span class="math inline">\(V\)</span> 拆分成两个子集 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span>，如果对于图中的任意一条边 <span class="math inline">\(e _ { i j }\)</span> 均有 <span class="math inline">\(v _ { i } \in A , \space v _ { j } \in B\)</span> 或者 <span class="math inline">\(v _ { i } \in B , \space v _ { j } \in A\)</span>，则称图 <span class="math inline">\(G\)</span> 为二部图。</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127115718.png" alt="二部图"><figcaption>二部图</figcaption></figure><p>二部图是一种十分常见的图数据对象，描述了两类对象之间的交互关系，比如：用户与商品、作者与论文。</p><h3 id="稀疏图和稠密图"><span class="exturl" data-url="aHR0cHM6Ly93d3cuZXB1Yml0LmNvbS9ib29rRGV0YWlscz9pZD1VQjZjYjQ1MDI0ZjhmMTE=" title="https://www.epubit.com/bookDetails?id=UB6cb45024f8f11">稀疏图和稠密图<i class="fa fa-external-link"></i></span></h3><p>通俗地说，如果边的数量接近于与顶点的数量呈线性关系，那么这个图就是稀疏图；如果边的数量接近于与顶点的数量呈平方关系，那么这个图就是稠密图。</p><h2 id="邻居和度">邻居和度</h2><h3 id="邻居">邻居</h3><p>如果存在一条边连接顶点 <span class="math inline">\(v _ { i }\)</span> 和 <span class="math inline">\(v _ { j }\)</span>，则称 <span class="math inline">\(v _ { \mathrm { j } }\)</span> 是 <span class="math inline">\(v _ { \mathrm { i } }\)</span> 的<strong>邻居</strong>，反之亦然。我们记 <span class="math inline">\(v _ { \mathrm { i } }\)</span> 的所有邻居为集合 <span class="math inline">\(N \left( v _ { i } \right)\)</span>，即</p><p><span class="math display">\[ N \left( v _ { i } \right) = \left\{ v _ { j } | \exists e _ { i j } \in E \text { or } e _ { j i } \in E \right\} \]</span></p><h3 id="度">度</h3><p>以 <span class="math inline">\(v _ { \mathrm { i } }\)</span> 为端点的边的数目成为 <span class="math inline">\(v _ { \mathrm { i } }\)</span> 的<strong>度</strong>（Degree），记为<span class="math inline">\(\operatorname { deg } \left( v _ { \mathrm {i}} \right)\)</span> ： <span class="math display">\[ \operatorname { deg } \left( v _ { i } \right) = \left| N \left( v _ { i } \right) \right| \]</span> 在图中，所有节点的度之和与边数存在如下关系： <span class="math display">\[ \sum _ { v _ { i } } \operatorname { deg } \left( v _ { i } \right) = 2 | E | \]</span></p><h3 id="出度和入度">出度和入度</h3><p>在有向图中还需要区分出度（Outdegree）和入度（Indegree），顶点的度数等于该顶点的出度与入度之和。</p><p>其中顶点 <span class="math inline">\(v _ { i }\)</span> 的出度是以 <span class="math inline">\(v _ { i }\)</span> 为起点的有向边的数目，顶点 <span class="math inline">\(v _ { i }\)</span> 的入度是以 <span class="math inline">\(v _ { i }\)</span> 为终点的有向边的数目。</p><h2 id="子图与路径">子图与路径</h2><h3 id="子图">子图</h3><p>若图 <span class="math inline">\(G&#39; = ( V&#39;,\space E&#39; )\)</span> 的顶点集和边集分别是另一个图 <span class="math inline">\(G = ( V ,\space E )\)</span> 的顶点集的子集和边集的子集，即 <span class="math inline">\(V&#39; \subseteq V\)</span>，且 <span class="math inline">\(E&#39; \subseteq E\)</span>，则称图 <span class="math inline">\(G&#39;\)</span> 是图 <span class="math inline">\(G\)</span> 的<strong>子图</strong>（Subgraph）。</p><h3 id="路径">路径</h3><p>在图 <span class="math inline">\(G = ( V,\space E )\)</span> 中，若从顶点 <span class="math inline">\(v _ { i }\)</span> 出发，沿着一些边经过一些顶点 <span class="math inline">\(v _ { p 1 } , v _ { p 2 } , \cdots , v _ { p m }\)</span>，到达顶点 <span class="math inline">\(v _ { j }\)</span>，则称边序列 <span class="math inline">\(P _ { i j } = \left( e _ { i p _ { 1 } } , e _ { p _ { 2 } p _ { 3 } } , \cdots , e _ { p _ { m } j } \right)\)</span> 为从顶点 <span class="math inline">\(v _ { i }\)</span> 到顶点 <span class="math inline">\(v _ { j }\)</span> 的一条路径（Path，也可称为通路），其中 <span class="math inline">\(e _ { i p _ { 1 } } , e _ { p _ { 2 } p _ { 3 } } , \cdots , e _ { p _ { m } j }\)</span> 为图 <span class="math inline">\(G\)</span> 中的边。</p><h4 id="路径的长度">路径的长度</h4><p>路径中边的数目通常称为路径的长度 $ L ( P _ { ij } ) = | P _ { ij } |$。</p><h4 id="顶点的距离">顶点的距离</h4><p>若存在至少一条路径由顶点 <span class="math inline">\(v _ { i }\)</span> 到达顶点 <span class="math inline">\(v _ { j }\)</span>，则定义 <span class="math inline">\(v _ { i }\)</span> 到 <span class="math inline">\(v _ { j }\)</span> 的距离为： <span class="math display">\[ d \left( v _ { i } , v _ { j } \right) = \min \left( \left| P _ { i j } \right| \right) \]</span> 即两个顶点之间的距离由它们的最短路径的长度决定。</p><p>我们设 <span class="math inline">\(d \left( v _ { i } , v _ { i } \right) = 0\)</span>，即<strong>节点到自身的距离为 0</strong>。</p><h4 id="k-阶邻居">k 阶邻居</h4><p>若 <span class="math inline">\(d\left( v _ { \text {i } } , v _ { j } \right) = k\)</span>，我们称 <span class="math inline">\(v _ { j }\)</span> 为 <span class="math inline">\(v _ { i }\)</span> 的 <strong>k 阶邻居</strong>。</p><h4 id="k-阶子图">k 阶子图</h4><p>我们称一个顶点 <span class="math inline">\(v _ { i }\)</span> 的 k 阶子图（k-subgraph）为： <span class="math display">\[ G _ { v _ { i } } ^ { ( k ) } = \left( V ^ { \prime } , E ^ { \prime } \right) , V ^ { \prime } = \left\{ v _ { j } | \forall v _ { j } , d \left( v _ { i } , v _ { j } \right) \leqslant k \right\} , E ^ { \prime } = \left\{ e _ { i j } | \forall v _ { j } , d \left( v _ { i } , v _ { j } \right) \leqslant k \right\} \]</span> 有时我们也称 k 阶子图为 k-hop，下图中的阴影部分就是顶点 <span class="math inline">\(v _ { 1 }\)</span> 的 2 阶子图：</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128103152.png" alt="图G的2阶子图"><figcaption>图G的2阶子图</figcaption></figure><h1 id="图的存储与遍历">图的存储与遍历</h1><h2 id="邻接矩阵与关联矩阵">邻接矩阵与关联矩阵</h2><h3 id="邻接矩阵">邻接矩阵</h3><p>设图 <span class="math inline">\(G = ( V,\space E )\)</span> ，在这里我们对边重新进行了编号 <span class="math inline">\(e _ { 1 } , e _ { 2 } , \ldots , e _ { M }\)</span>，如下图所示：</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128105325.png" alt="图G示例"><figcaption>图G示例</figcaption></figure><p>我们用邻接矩阵 A 描述图中顶点之间的关联，<span class="math inline">\(A \in R ^ { N \times N }\)</span>，其定义为： <span class="math display">\[ A _ { i j } = \left\{ \begin{array} { l } { 1 \text { if } \left( v _ { i } , v _ { j } \right) \subseteq E } \\ { 0 \text { else } } \end{array} \right. \]</span> 用邻接矩阵（Adjacency matrix）存储图的时候，我们需要一个一维数组表示顶点集合，需要一个二维数组表示邻接矩阵。</p><p>需要特别说明的是，由于在实际的图数据中，邻接矩阵往往会出现大量的 0 值，因此可以用<strong>稀疏矩阵</strong>的格式来存储邻接矩阵，这样可以将邻接矩阵的空间复杂度控制在 <span class="math inline">\(\mathrm { O } ( \mathrm { M } )\)</span> 的范围内。</p><p>图 <span class="math inline">\(G\)</span> 的邻接矩阵存储表示：</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128110458.png" alt="邻接矩阵"><figcaption>邻接矩阵</figcaption></figure><p>通过该图可以看出，无向图的邻接矩阵是沿主对角线对称的，即 <span class="math inline">\(A _ { i j } = A _ { j i }\)</span>。</p><h3 id="关联矩阵">关联矩阵</h3><p>除了邻接矩阵外，我们有时也用关联矩阵 <span class="math inline">\(\mathrm { B } \in \mathrm { R } ^ { \mathrm { N } \times \mathrm { M } }\)</span> 来描述节点与边之间的关联，定义如下： <span class="math display">\[ B _ { i j } = \left\{ \begin{array} { l } { 1 \text { if } v _ { i } \text { 与 } e _ { j } \text { 相连 } } \\ { 0 \text { else } } \end{array} \right. \]</span> 同样，关联矩阵（Incidence matrix）也可以用稀疏矩阵来存储，这是因为B的任意一列仅有两个非 0 值。</p><p>图 <span class="math inline">\(G\)</span> 的关联矩阵存储表示：</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128111838.png" alt="关联矩阵"><figcaption>关联矩阵</figcaption></figure><h2 id="图的遍历">图的遍历</h2><p>图的遍历是指从图中的某一顶点出发，按照某种搜索算法沿着图中的边对图中的所有顶点访问一次且仅访问一次。</p><p>图的遍历主要有两种算法：深度优先搜索（DFS，Depth-First-Search）和广度优先搜索（BFS，Breadth-First-Search）。</p><h3 id="深度优先搜索">深度优先搜索</h3><p>深度优先搜索是一个递归算法，有回退过程。</p><p>其算法思想是：由图中某一顶点 <span class="math inline">\(v _ { i }\)</span> 出发，访问它的任意一个邻居 <span class="math inline">\(w _ { 1 }\)</span>；再从 <span class="math inline">\(w _ { 1 }\)</span> 出发，访问 <span class="math inline">\(w _ { 1 }\)</span> 的所有邻居中未被访问过的顶点 <span class="math inline">\(w _ { 2 }\)</span>；然后再从 <span class="math inline">\(w _ { 2 }\)</span> 出发，依次访问，直到出现某顶点不再有邻居未被访问过。接着，回退一步，回退到前一次刚访问过的顶点，看是否还有其他未被访问过的邻居，如果有，则访问该邻居，之后再从该邻居出发，进行与前面类似的访问；如果没有，就再回退一步进行类似访问。重复上述过程，直到该图中所有顶点都被访问过为止。</p><h3 id="广度优先搜索">广度优先搜索</h3><p>广度优先搜索是一个分层的搜索过程，没有回退过程。</p><p>其算法思想是：从图中某一顶点 <span class="math inline">\(v _ { i }\)</span> 开始，由 <span class="math inline">\(v _ { i }\)</span> 出发，依次访问 <span class="math inline">\(v _ { i }\)</span> 的所有未被访问过的邻居 <span class="math inline">\(w _ { 1 } , w _ { 2 } , \ldots , w _ { n }\)</span>；然后再顺序访问 <span class="math inline">\(w _ { 1 } , w _ { 2 } , \ldots , w _ { n }\)</span> 的所有还未被访问过的邻居，如此一层层执行下去，直到图中所有顶点都被访问到为止。</p><h1 id="图数据的应用场景">图数据的应用场景</h1><p>在实际的数据场景中，我们通常将图称为网络（Network），与之对应的，图的两个要素（顶点和边）也被称为节点（Node）和关系（Link），比如我们熟知的社交网络、物流网络等概念名词。</p><h2 id="图数据的类型">图数据的类型</h2><h3 id="同构图">同构图</h3><p>同构图（Homogeneous Graph）是指图中的节点类型和关系类型都仅有一种。</p><p>同构图是实际图数据的一种最简化的情况，如由超链接关系所构成的万维网，这类图数据的信息全部包含在邻接矩阵里。</p><h3 id="异构图">异构图</h3><p>与同构图相反，异构图（Heterogeneous Graph）是指图中的节点类型或关系类型多于一种。</p><p>在现实场景中，我们通常研究的图数据对象是多类型的，对象之间的交互关系也是多样化的。因此，异构图能够更好地贴近现实。</p><h3 id="属性图">属性图</h3><p>相较于异构图，属性图（Property Graph）给图数据增加了额外的属性信息，如图所示：</p><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128171828.png" alt="属性图"><figcaption>属性图</figcaption></figure><p>对于一个属性图而言，节点和关系都有标签（Label）和属性（Property），这里的标签是指节点或关系的类型，如某节点的类型为“用户”，属性是节点或关系的附加描述信息，如“用户”节点可以有“姓名”“注册时间”“注册地址”等属性。</p><p>属性图是一种最常见的工业级图数据的表示方式，能够广泛适用于多种业务场景下的数据表达。</p><h3 id="非显式图">非显式图</h3><p>非显式图（Graph Constructed from Non-relational Data）是指数据之间没有显式地定义出关系，需要依据某种规则或计算方式将数据的关系表达出来，进而将数据当成一种图数据进行研究。</p><p>比如计算机3D视觉中的点云数据，如果我们将节点之间的空间距离转化成关系的话，点云数据就成了图数据。</p><h2 id="图数据应用示例">图数据应用示例</h2><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128172817.png" alt="图数据应用示例"><figcaption>图数据应用示例</figcaption></figure><h3 id="社交网络">社交网络</h3><p>社交网络是十分常见的一类图数据，代表着各种个人或组织之间的社会关系。</p><p>上图中的 a 图展示了在线社交网络中的用户关注网络：以用户为节点，用户之间的关注关系作为边。</p><p>这是一个典型的同构图，一般用来研究用户的重要性排名以及相关的用户推荐等问题。随着移动互联网技术的不断深入，更多元化的媒体对象被补充进社交网络中，比如短文本、视频等，如此构成的异构图可以完成更加多样化的任务。</p><h3 id="电子购物">电子购物</h3><p>电子购物是互联网中的一类核心业务，在这类场景中，业务数据通常可以用一个用户–商品的二部图来描述。</p><p>在上图中的 b 图所展示的例子中，节点分为两类：用户和商品，存在的关系有浏览、收藏、购买等。用户与商品之间可以存在多重关系，如既存在收藏关系也存在购买关系。这类复杂的数据场景可以用属性图轻松描述。</p><p>电子购物催生了一项大家熟知的技术应用—推荐系统。用户与商品之间的交互关系，反映了用户的购物偏好。例如，经典的啤酒与尿布的故事：爱买啤酒的人通常也更爱买尿布。</p><h3 id="化学分子">化学分子</h3><p>以原子为节点，原子之间的化学键作为边，我们可以将分子视为一种图数据进行研究，分子的基本构成以及内在联系决定了分子的各项理化性质，通常我们用其指导新材料、新药物的研究任务，如上图中的 c 图所示。</p><h3 id="交通网络">交通网络</h3><p>交通网络具有多种形式，比如地铁网络中将各个站点作为节点，站点之间的连通性作为边构成一张图，如上图中的 d 图所示。</p><p>通常在交通网络中我们比较关注的是路径规划相关的问题：比如最短路径问题，再如我们将车流量作为网络中节点的属性，去预测未来交通流量的变化情况。</p><h3 id="场景图">场景图</h3><p>场景图是图像语义的一种描述方式，它将图像中的物体当作节点，物体之间的相互关系当作边构成一张图。场景图可以将关系复杂的图像简化成一个关系明确的语义图。</p><p>场景图具有十分强大的应用场景，如图像合成、图像语义检索、视觉推理等。上图中的 e 图为由场景图合成相关语义图像的示例，在该场景图中，描述了 5 个对象：两个男人、一个小孩、飞盘、庭院以及他们之间的关系，可以看到场景图具有很强的语义表示能力。</p><h3 id="电路设计图">电路设计图</h3><p>我们可以将电子器件如谐振器作为节点，器件之间的布线作为边将电路设计抽象成一种图数据。</p><p>在参考文献<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUNpcmN1aXQtR05OJUVGJUJDJTlBR3JhcGgrTmV1cmFsK05ldHdvcmtzK2ZvcitEaXN0cmlidXRlZCtDaXJjdWl0K0Rlc2lnbiZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Circuit-GNN%EF%BC%9AGraph+Neural+Networks+for+Distributed+Circuit+Design&amp;btnG=">[1]<i class="fa fa-external-link"></i></span>中，对电路设计进行了这样的抽象，如上图中的 f 图所示，然后基于图神经网络技术对电路的电磁特性进行仿真拟合，相较于严格的电磁学公式仿真，可以在可接受的误差范围内极大地加速高频电路的设计工作。</p><h1 id="图数据深度学习">图数据深度学习</h1><h2 id="基于图数据相关的任务学习理论">基于图数据相关的任务学习理论</h2><h3 id="谱图理论">谱图理论</h3><p>谱图理论（Spectral Graph Theory）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNwZWN0cmFsK0dyYXBoK1RoZW9yeStBbWVyaWNhbitNYXRoZW1hdGljYWwrU29jaWV0eSZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Spectral+Graph+Theory+American+Mathematical+Society&amp;btnG=">[2]<i class="fa fa-external-link"></i></span>是将图论与线性代数相结合的理论，基于此理论发展而来的谱聚类相关算法<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErdHV0b3JpYWwrb24rc3BlY3RyYWwrY2x1c3RlcmluZyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+tutorial+on+spectral+clustering&amp;btnG=">[3]<i class="fa fa-external-link"></i></span>，可以用来解决图的分割或者节点的聚类问题。</p><h3 id="统计关系学习">统计关系学习</h3><p>统计关系学习（Statistical Relational Learning）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUludHJvZHVjdGlvbit0bytzdGF0aXN0aWNhbCtyZWxhdGlvbmFsK2xlYXJuaW5nJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Introduction+to+statistical+relational+learning&amp;btnG=">[4]<i class="fa fa-external-link"></i></span>是将关系表示与似然表示相结合的机器学习理论。</p><p>区别于传统的机器学习算法对数据独立同分布（independent and Identically Distributed，数据对象是同类且独立不相关的）的假设，统计关系学习打破了对数据的上述两种假设，对图数据的学习具有更好的契合度。</p><h3 id="异构信息网络">异构信息网络</h3><p>为了更加贴合实际场景中的异构图数据，异构信息网络（Heterogeneous Information Network）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErc3VydmV5K29mK2hldGVyb2dlbmVvdXMraW5mb3JtYXRpb24rbmV0d29yaythbmFseXNpcyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+survey+of+heterogeneous+information+network+analysis&amp;btnG=">[5]<i class="fa fa-external-link"></i></span>分析被提出，用以挖掘异构图中更加全面的结构信息和丰富的语义信息。</p><h3 id="网络表示学习">网络表示学习</h3><p>由于这些年深度学习在实际应用领域取得的巨大成就，表示学习和端对端学习的概念日益得到重视，为了从复杂的图数据中学习到包含充分信息的向量化表示，出现了大量网络表示学习（Network Embedding）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErc3VydmV5K29uK25ldHdvcmsrZW1iZWRkaW5nJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+survey+on+network+embedding&amp;btnG=">[6]<i class="fa fa-external-link"></i></span>的方法。</p><p>然而网络表示学习很难提供表示学习加任务学习的端对端系统，基于此，图数据的端对端学习系统仍然是一个重要的研究课题。</p><h2 id="与图数据相对应的数据">与图数据相对应的数据</h2><figure><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128220741.png" alt="图像和语音文本数据类型"><figcaption>图像和语音文本数据类型</figcaption></figure><p>与图数据相对应的数据有图像、语音与文本，这些数据是定义在欧式空间中的规则化结构数据，基于这些数据的张量计算体系是比较自然且高效的。</p><h3 id="图像数据">图像数据</h3><p>图像数据呈现出规则的 2D 栅格结构，这种栅格结构与卷积神经网络的作用机制具有良好的对应。</p><h3 id="语音数据"><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNDcwMzI2OA==" title="https://zhuanlan.zhihu.com/p/24703268">语音数据<i class="fa fa-external-link"></i></span></h3><p>声波是二维的，它在每个时刻都有一个基于其高度的值。为了将这个声波转换成数字，我们只记录声波在等距点的高度，这就是采样（sampling）。可以使用循环神经网络处理语音数据，因为它预测的每个字母都应该能够影响它对下一个字母的预测。</p><h3 id="文本数据">文本数据</h3><p>文本数据是一种规则的序列数据，这种序列结构与循环神经网络的作用机制相对应。</p><h2 id="基于图卷积操作的神经网络理论">基于图卷积操作的神经网络理论</h2><p>受图信号处理（Graph Signal Processing）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVRoZStlbWVyZ2luZytmaWVsZCtvZitzaWduYWwrcHJvY2Vzc2luZytvbitncmFwaHMlRUYlQkMlOUFFeHRlbmRpbmcraGlnaC1kaW1lbnNpb25hbCtkYXRhK2FuYWx5c2lzK3RvK25ldHdvcmtzK2FuZCtvdGhlcitpcnJlZ3VsYXIrZG9tYWlucyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=The+emerging+field+of+signal+processing+on+graphs%EF%BC%9AExtending+high-dimensional+data+analysis+to+networks+and+other+irregular+domains&amp;btnG=">[7]<i class="fa fa-external-link"></i></span>中对图信号卷积滤波的定义的启发，近几年发展出了一套基于图卷积操作并不断衍生的神经网络理论。<span class="exturl" data-url="aHR0cHM6Ly9pdGVtLmpkLmNvbS8xMjYxNTA2NS5odG1s" title="https://item.jd.com/12615065.html">本书<i class="fa fa-external-link"></i></span>将这类方法统称为图神经网络（Graph Neural Network，GNN<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUdyYXBoK25ldXJhbCtuZXR3b3JrcyVFRiVCQyU5QUErcmV2aWV3K29mK21ldGhvZHMrYW5kK2FwcGxpY2F0aW9ucyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Graph+neural+networks%EF%BC%9AA+review+of+methods+and+applications&amp;btnG=">[8]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPURlZXArbGVhcm5pbmcrb24rZ3JhcGhzJUVGJUJDJTlBQStzdXJ2ZXkmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Deep+learning+on+graphs%EF%BC%9AA+survey&amp;btnG=">[9]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErY29tcHJlaGVuc2l2ZStzdXJ2ZXkrb24rZ3JhcGgrbmV1cmFsK25ldHdvcmtzJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+comprehensive+survey+on+graph+neural+networks&amp;btnG=">[10]<i class="fa fa-external-link"></i></span>）。</p><p>2005 年，Marco Gori 等人发表论文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErbmV3K21vZGVsK2ZvcitsZWFybmluZytpbitncmFwaCtkb21haW5zJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+new+model+for+learning+in+graph+domains&amp;btnG=">[11]<i class="fa fa-external-link"></i></span>，首次提出了图神经网络的概念。在此之前，处理图数据的方法是在数据的预处理阶段将图转换为用一组向量表示。这种处理方法最大的问题就是图中的结构信息可能会丢失，并且得到的结果会严重依赖于对图的预处理。GNN 的提出，便是为了能够将学习过程直接架构于图数据之上。</p><p>随后，其在 2009 年的两篇论文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPU5ldXJhbCtuZXR3b3JrK2ZvcitncmFwaHMlRUYlQkMlOUFBK2NvbnRleHR1YWwrY29uc3RydWN0aXZlK2FwcHJvYWNoJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Neural+network+for+graphs%EF%BC%9AA+contextual+constructive+approach&amp;btnG=">[12]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVRoZStncmFwaCtuZXVyYWwrbmV0d29yayttb2RlbCZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=The+graph+neural+network+model&amp;btnG=">[13]<i class="fa fa-external-link"></i></span>中又进一步阐述了图神经网络，并提出了一种监督学习的方法来训练 GNN。但是，早期的这些研究都是以迭代的方式，通过循环神经网络传播邻居信息，直到达到稳定的固定状态来学习节点的表示。这种计算方式消耗非常大，相关研究开始关注如何改进这种方法以减小计算量。</p><p>2012年前后，卷积神经网络开始在视觉领域取得令人瞩目的成绩，于是人们开始考虑如何将卷积应用到图神经网络中。2013 年 Bruna 等人首次将卷积引入图神经网络中，在引文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNwZWN0cmFsK25ldHdvcmtzK2FuZCtsb2NhbGx5K2Nvbm5lY3RlZCtuZXR3b3JrcytvbitncmFwaHMmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Spectral+networks+and+locally+connected+networks+on+graphs&amp;btnG=">[14]<i class="fa fa-external-link"></i></span>中基于频域卷积操作的概念开发了一种图卷积网络模型，首次将可学习的卷积操作用于图数据之上。自此以后，不断有人提出改进、拓展这种基于频域图卷积的神经网络模型。但是基于频域卷积的方法在计算时需要同时处理整个图，并且需要承担矩阵分解时的很高的时间复杂度，这很难使学习系统扩展到大规模图数据的学习任务上去，所以基于空域的图卷积被提出并逐渐流行。</p><p>2016年，Kipf 等人<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNlbWktc3VwZXJ2aXNlZCtjbGFzc2lmaWNhdGlvbit3aXRoK2dyYXBoK2NvbnZvbHV0aW9uYWwrbmV0d29ya3MmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Semi-supervised+classification+with+graph+convolutional+networks&amp;btnG=">[15]<i class="fa fa-external-link"></i></span>将频域图卷积的定义进行简化，使得图卷积的操作能够在空域进行，这极大地提升了图卷积模型的计算效率，同时，得益于卷积滤波的高效性，图卷积模型在多项图数据相关的任务上取得了令人瞩目的成绩。</p><p>近几年，更多的基于空域图卷积的神经网络模型的变体<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUluZHVjdGl2ZStyZXByZXNlbnRhdGlvbitsZWFybmluZytvbitsYXJnZStncmFwaHMmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Inductive+representation+learning+on+large+graphs&amp;btnG=">[16]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUdyYXBoK2F0dGVudGlvbituZXR3b3JrcyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Graph+attention+networks&amp;btnG=">[17]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPU5ldXJhbCttZXNzYWdlK3Bhc3NpbmcrZm9yK3F1YW50dW0rY2hlbWlzdHJ5JmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Neural+message+passing+for+quantum+chemistry&amp;btnG=">[18]<i class="fa fa-external-link"></i></span>被开发出来，我们将这类方法统称为 GNN。各种 GNN 模型的出现，大大加强了学习系统对各类图数据的适应性，这也为各种图数据的任务学习奠定了坚实的基础。</p><p>自此，图数据与深度学习有了第一次真正意义上的结合。GNN 的出现，实现了图数据的端对端学习方式，为图数据的诸多应用场景下的任务，提供了一个极具竞争力的学习方案。</p><h2 id="图数据相关任务的分类">图数据相关任务的分类</h2><h3 id="节点层面的任务">节点层面的任务</h3><p>节点层面（Node Level）的任务主要包括分类任务和回归任务。这类任务虽然是对节点层面的性质进行预测，但是显然不应该将模型建立在一个个单独的节点上，节点的关系也需要考虑。节点层面的任务有很多，包括学术上使用较多的对论文引用网络中的论文节点进行分类，工业界在线社交网络中用户标签的分类、恶意账户检测等。</p><h3 id="边层面的任务">边层面的任务</h3><p>边层面（Link Level）的任务主要包括边的分类和预测任务。边的分类是指对边的某种性质进行预测；边预测是指给定的两个节点之间是否会构成边。常见的应用场景比如在社交网络中，将用户作为节点，用户之间的关注关系建模为边，通过边预测实现社交用户的推荐。目前，边层面的任务主要集中在推荐业务中。</p><h3 id="图层面的任务">图层面的任务</h3><p>图层面（Graph Level）的任务不依赖于某个节点或者某条边的属性，而是从图的整体结构出发，实现分类、表示和生成等任务。目前，图层面的任务主要应用在自然科学研究领域，比如对药物分子的分类、酶的分类等。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E5%9B%BE%E8%AE%BA/" rel="tag"># 图论</a> <a href="/tags/Algorithm/" rel="tag"># Algorithm</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/01/23/%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E6%9C%80%E4%BD%B3%E8%B7%AF%E5%BE%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/" rel="prev" title="学习数据挖掘的最佳路径是什么（笔记）"><i class="fa fa-chevron-left"></i> 学习数据挖掘的最佳路径是什么（笔记）</a></div><div class="post-nav-item"></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#图的概述"><span class="nav-number">1.</span> <span class="nav-text">图的概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图的基本定义"><span class="nav-number">2.</span> <span class="nav-text">图的基本定义</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#图的基本类型"><span class="nav-number">2.1.</span> <span class="nav-text">图的基本类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#有向图和无向图"><span class="nav-number">2.1.1.</span> <span class="nav-text">有向图和无向图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非加权图与加权图"><span class="nav-number">2.1.2.</span> <span class="nav-text">非加权图与加权图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#连通图与非连通图"><span class="nav-number">2.1.3.</span> <span class="nav-text">连通图与非连通图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二部图"><span class="nav-number">2.1.4.</span> <span class="nav-text">二部图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#稀疏图和稠密图"><span class="nav-number">2.1.5.</span> <span class="nav-text">稀疏图和稠密图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#邻居和度"><span class="nav-number">2.2.</span> <span class="nav-text">邻居和度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#邻居"><span class="nav-number">2.2.1.</span> <span class="nav-text">邻居</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#度"><span class="nav-number">2.2.2.</span> <span class="nav-text">度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#出度和入度"><span class="nav-number">2.2.3.</span> <span class="nav-text">出度和入度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#子图与路径"><span class="nav-number">2.3.</span> <span class="nav-text">子图与路径</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#子图"><span class="nav-number">2.3.1.</span> <span class="nav-text">子图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#路径"><span class="nav-number">2.3.2.</span> <span class="nav-text">路径</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#路径的长度"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">路径的长度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#顶点的距离"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">顶点的距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-阶邻居"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">k 阶邻居</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-阶子图"><span class="nav-number">2.3.2.4.</span> <span class="nav-text">k 阶子图</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图的存储与遍历"><span class="nav-number">3.</span> <span class="nav-text">图的存储与遍历</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#邻接矩阵与关联矩阵"><span class="nav-number">3.1.</span> <span class="nav-text">邻接矩阵与关联矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#邻接矩阵"><span class="nav-number">3.1.1.</span> <span class="nav-text">邻接矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关联矩阵"><span class="nav-number">3.1.2.</span> <span class="nav-text">关联矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图的遍历"><span class="nav-number">3.2.</span> <span class="nav-text">图的遍历</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度优先搜索"><span class="nav-number">3.2.1.</span> <span class="nav-text">深度优先搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广度优先搜索"><span class="nav-number">3.2.2.</span> <span class="nav-text">广度优先搜索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图数据的应用场景"><span class="nav-number">4.</span> <span class="nav-text">图数据的应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#图数据的类型"><span class="nav-number">4.1.</span> <span class="nav-text">图数据的类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#同构图"><span class="nav-number">4.1.1.</span> <span class="nav-text">同构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#异构图"><span class="nav-number">4.1.2.</span> <span class="nav-text">异构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#属性图"><span class="nav-number">4.1.3.</span> <span class="nav-text">属性图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非显式图"><span class="nav-number">4.1.4.</span> <span class="nav-text">非显式图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图数据应用示例"><span class="nav-number">4.2.</span> <span class="nav-text">图数据应用示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#社交网络"><span class="nav-number">4.2.1.</span> <span class="nav-text">社交网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#电子购物"><span class="nav-number">4.2.2.</span> <span class="nav-text">电子购物</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#化学分子"><span class="nav-number">4.2.3.</span> <span class="nav-text">化学分子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交通网络"><span class="nav-number">4.2.4.</span> <span class="nav-text">交通网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#场景图"><span class="nav-number">4.2.5.</span> <span class="nav-text">场景图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#电路设计图"><span class="nav-number">4.2.6.</span> <span class="nav-text">电路设计图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#图数据深度学习"><span class="nav-number">5.</span> <span class="nav-text">图数据深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基于图数据相关的任务学习理论"><span class="nav-number">5.1.</span> <span class="nav-text">基于图数据相关的任务学习理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#谱图理论"><span class="nav-number">5.1.1.</span> <span class="nav-text">谱图理论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#统计关系学习"><span class="nav-number">5.1.2.</span> <span class="nav-text">统计关系学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#异构信息网络"><span class="nav-number">5.1.3.</span> <span class="nav-text">异构信息网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络表示学习"><span class="nav-number">5.1.4.</span> <span class="nav-text">网络表示学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#与图数据相对应的数据"><span class="nav-number">5.2.</span> <span class="nav-text">与图数据相对应的数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图像数据"><span class="nav-number">5.2.1.</span> <span class="nav-text">图像数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语音数据"><span class="nav-number">5.2.2.</span> <span class="nav-text">语音数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本数据"><span class="nav-number">5.2.3.</span> <span class="nav-text">文本数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于图卷积操作的神经网络理论"><span class="nav-number">5.3.</span> <span class="nav-text">基于图卷积操作的神经网络理论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图数据相关任务的分类"><span class="nav-number">5.4.</span> <span class="nav-text">图数据相关任务的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#节点层面的任务"><span class="nav-number">5.4.1.</span> <span class="nav-text">节点层面的任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#边层面的任务"><span class="nav-number">5.4.2.</span> <span class="nav-text">边层面的任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图层面的任务"><span class="nav-number">5.4.3.</span> <span class="nav-text">图层面的任务</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Morgan" src="/images/apple-touch-icon-next.png"><p class="site-author-name" itemprop="name">Morgan</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xzajIxMDAwMQ==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lsj210001"><i class="fa fa-fw fa-github"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOmR0Ymxsc2pAZ21haWwuY29t" title="E-Mail → mailto:dtbllsj@gmail.com"><i class="fa fa-fw fa-envelope"></i></span> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXR0ZXIuaW0vdGhlbWUtbmV4dA==" title="Gitter → https:&#x2F;&#x2F;gitter.im&#x2F;theme-next"><i class="fa fa-fw fa-github-alt"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9yaW90LmltL2FwcC8jL3Jvb20vI05leFQ6bWF0cml4Lm9yZw==" title="Riot → https:&#x2F;&#x2F;riot.im&#x2F;app&#x2F;#&#x2F;room&#x2F;#NexT:matrix.org"><i class="fa fa-fw fa-bullhorn"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL2pvaW5jaGF0L0dVTkhYQS12WmtnU011aW1MMVZtTXc=" title="Telegram [Chat] → https:&#x2F;&#x2F;t.me&#x2F;joinchat&#x2F;GUNHXA-vZkgSMuimL1VmMw"><i class="fa fa-fw fa-telegram"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL3RoZW1lX25leHQ=" title="Telegram [News] → https:&#x2F;&#x2F;t.me&#x2F;theme_next"><i class="fa fa-fw fa-paper-plane"></i></span></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span></div><div class="cc-license motion-element" itemprop="sponsor"><span class="exturl cc-opacity" title="Deploy with Netlify → https://www.netlify.com" data-url="aHR0cHM6Ly93d3cubmV0bGlmeS5jb20="><img width="80" src="https://www.netlify.com/img/global/badges/netlify-dark.svg" alt="Netlify"></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2014 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">NexT</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">20k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">18 分钟</span></div><div class="footer-custom">Theme source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0">here</span><br>Website source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmc=">here</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>function loadCount(){var t=document,d=t.createElement("script");d.src="https://theme-next.disqus.com/count.js",d.id="dsq-count-scr",(t.head||t.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://lsj210001.github.io/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/";
    this.page.identifier = "2020/01/29/图的概述（笔记）/";
    this.page.title = "图的概述（笔记）";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://theme-next.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html><!-- rebuild by neat -->