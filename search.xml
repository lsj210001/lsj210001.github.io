<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>图的概述（笔记）</title>
    <url>/2020/01/29/%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><blockquote><p>本文为<span class="exturl" data-url="aHR0cHM6Ly9pdGVtLmpkLmNvbS8xMjYxNTA2NS5odG1s" title="https://item.jd.com/12615065.html">《深入浅出图神经网络：GNN原理解析》<i class="fa fa-external-link"></i></span>第一章的笔记</p></blockquote><h1 id="图的概述"><a class="header-anchor" href="#图的概述"></a>图的概述</h1><p><strong>图</strong>（Graph）是一个具有广泛含义的对象：</p><ul><li>在数学中，图是图论的主要研究对象；</li><li>在计算机工程领域，图是一种常见的数据结构；</li><li>在数据科学中，图被用来广泛描述各类关系型数据。</li></ul><p>通常，图被用来表示<strong>物体与物体之间的关系</strong>，例如：</p><ul><li>化学分子</li><li>通信网络</li><li>社交网络</li></ul><p>事实上，任何一个包含<strong>二元关系</strong>的系统都可以用图来描述。</p><h1 id="图的基本定义"><a class="header-anchor" href="#图的基本定义"></a>图的基本定义</h1><p>图可以表示为<strong>顶点</strong>（Vertex）和连接顶点的<strong>边</strong>（Edge）的集合，记为 $G = ( V ,\space E )$，其中 $V$ 是顶点集合，$E$ 是边集合。</p><p>我们可设图 $G$ 的顶点数为 $N$，边数为 $M$，一条连接顶点 $v _ { i } , \space v _ { j } \in V$ 的边记为 $\left( v _ { i } , \space v _ { j } \right)$ 或者 $e _ { i j }$。</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200126234551.png" alt="图G的定义"></p><h2 id="图的基本类型"><a class="header-anchor" href="#图的基本类型"></a>图的基本类型</h2><h3 id="有向图和无向图"><a class="header-anchor" href="#有向图和无向图"></a>有向图和无向图</h3><p>如果图中的边存在方向性，则称这样的边为有向边 $e _ { ij } = &lt; v _ { i } , \space v _ { j } &gt;$，其中 $v _ { i }$ 是这条有向边的起点，$v _ { j }$ 是这条有向边的终点，包含有向边的图称为有向图。</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127000108.png" alt="有向图"></p><p>与有向图相对应的是无向图，无向图中的边都是无向边，我们可以认为无向边是对称的，同时包含两个方向：$e _ { ij } = &lt; v _ { i } , \space v _ { j } &gt; = &lt; v _ { j } , \space v _ { i } &gt; = e _ { ji }$。</p><h3 id="非加权图与加权图"><a class="header-anchor" href="#非加权图与加权图"></a>非加权图与加权图</h3><p>如果图里的每条边都有一个实数与之对应，我们称这样的图为加权图，该实数称为对应边上的权重。</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127110459.png" alt="加权图"></p><p>在实际场景中，权重可以代表两地之间的路程或运输成本。一般情况下，我们习惯把权重抽象成两个顶点之间的<strong>连接强度</strong>。与之相反的是非加权图，我们可以认为非加权图各边上的权重是一样的。</p><h3 id="连通图与非连通图"><a class="header-anchor" href="#连通图与非连通图"></a>连通图与非连通图</h3><p>如果图中存在<strong>孤立的顶点</strong>，没有任何边与之相连，这样的图被称为非连通图。（下图中应该有一个孤立顶点 $v _ { 5 }$ 没有绘制出来）</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127111035.png" alt="非连通图"></p><p>图中不存在孤立顶点的图称为连通图。</p><h3 id="二部图"><a class="header-anchor" href="#二部图"></a>二部图</h3><p>二部图是一类特殊的图。我们将 $G$ 中的顶点集合 $V$ 拆分成两个子集 $A$ 和 $B$，如果对于图中的任意一条边 $e _ { i j }$ 均有 $v _ { i } \in A , \space v _ { j } \in B$ 或者 $v _ { i } \in B , \space v _ { j } \in A$，则称图 $G$ 为二部图。</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200127115718.png" alt="二部图"></p><p>二部图是一种十分常见的图数据对象，描述了两类对象之间的交互关系，比如：用户与商品、作者与论文。</p><h3 id="稀疏图和稠密图"><a class="header-anchor" href="#稀疏图和稠密图"></a><span class="exturl" data-url="aHR0cHM6Ly93d3cuZXB1Yml0LmNvbS9ib29rRGV0YWlscz9pZD1VQjZjYjQ1MDI0ZjhmMTE=" title="https://www.epubit.com/bookDetails?id=UB6cb45024f8f11">稀疏图和稠密图<i class="fa fa-external-link"></i></span></h3><p>通俗地说，如果边的数量接近于与顶点的数量呈线性关系，那么这个图就是稀疏图；如果边的数量接近于与顶点的数量呈平方关系，那么这个图就是稠密图。</p><h2 id="邻居和度"><a class="header-anchor" href="#邻居和度"></a>邻居和度</h2><h3 id="邻居"><a class="header-anchor" href="#邻居"></a>邻居</h3><p>如果存在一条边连接顶点 $v _ { i }$ 和 $v _ { j }$，则称 $v _ { \mathrm { j } }$ 是 $v _ { \mathrm { i } }$ 的<strong>邻居</strong>，反之亦然。我们记 $v _ { \mathrm { i } }$ 的所有邻居为集合 $N \left( v _ { i } \right)$，即</p><p>$$<br>N \left( v _ { i } \right) = \left\{ v _ { j } | \exists e _ { i j } \in E \text { or } e _ { j i } \in E \right\}<br>$$</p><h3 id="度"><a class="header-anchor" href="#度"></a>度</h3><p>以 $v _ { \mathrm { i } }$ 为端点的边的数目成为 $v _ { \mathrm { i } }$ 的<strong>度</strong>（Degree），记为$\operatorname { deg } \left( v _ { \mathrm {i}} \right)$ ：<br>$$<br>\operatorname { deg } \left( v _ { i } \right) = \left| N \left( v _ { i } \right) \right|<br>$$<br>在图中，所有节点的度之和与边数存在如下关系：<br>$$<br>\sum _ { v _ { i } } \operatorname { deg } \left( v _ { i } \right) = 2 | E |<br>$$</p><h3 id="出度和入度"><a class="header-anchor" href="#出度和入度"></a>出度和入度</h3><p>在有向图中还需要区分出度（Outdegree）和入度（Indegree），顶点的度数等于该顶点的出度与入度之和。</p><p>其中顶点 $v _ { i }$ 的出度是以 $v _ { i }$ 为起点的有向边的数目，顶点 $v _ { i }$ 的入度是以 $v _ { i }$ 为终点的有向边的数目。</p><h2 id="子图与路径"><a class="header-anchor" href="#子图与路径"></a>子图与路径</h2><h3 id="子图"><a class="header-anchor" href="#子图"></a>子图</h3><p>若图 $G’ = ( V’,\space E’ )$ 的顶点集和边集分别是另一个图 $G = ( V ,\space E )$ 的顶点集的子集和边集的子集，即 $V’ \subseteq V$，且 $E’ \subseteq E$，则称图 $G’$ 是图 $G$ 的<strong>子图</strong>（Subgraph）。</p><h3 id="路径"><a class="header-anchor" href="#路径"></a>路径</h3><p>在图 $G = ( V,\space E )$ 中，若从顶点 $v _ { i }$ 出发，沿着一些边经过一些顶点 $v _ { p 1 } , v _ { p 2 } , \cdots , v _ { p m }$，到达顶点 $v _ { j }$，则称边序列 $P _ { i j } = \left( e _ { i p _ { 1 } } , e _ { p _ { 2 } p _ { 3 } } , \cdots , e _ { p _ { m } j } \right)$ 为从顶点 $v _ { i }$ 到顶点 $v _ { j }$ 的一条路径（Path，也可称为通路），其中 $e _ { i p _ { 1 } } , e _ { p _ { 2 } p _ { 3 } } , \cdots , e _ { p _ { m } j }$ 为图 $G$ 中的边。</p><h4 id="路径的长度"><a class="header-anchor" href="#路径的长度"></a>路径的长度</h4><p>路径中边的数目通常称为路径的长度 $ L \left( P _ { ij } \right) = \left| P _ { ij } \right|$。</p><h4 id="顶点的距离"><a class="header-anchor" href="#顶点的距离"></a>顶点的距离</h4><p>若存在至少一条路径由顶点 $v _ { i }$ 到达顶点 $v _ { j }$，则定义 $v _ { i }$ 到 $v _ { j }$ 的距离为：<br>$$<br>d \left( v _ { i } , v _ { j } \right) = \min \left( \left| P _ { i j } \right| \right)<br>$$<br>即两个顶点之间的距离由它们的最短路径的长度决定。</p><p>我们设 $d \left( v _ { i } , v _ { i } \right) = 0$，即<strong>节点到自身的距离为 0</strong>。</p><h4 id="k-阶邻居"><a class="header-anchor" href="#k-阶邻居"></a>k 阶邻居</h4><p>若 $d\left( v _ { \text {i } } , v _ { j } \right) = k$，我们称 $v _ { j }$ 为 $v _ { i }$ 的 <strong>k 阶邻居</strong>。</p><h4 id="k-阶子图"><a class="header-anchor" href="#k-阶子图"></a>k 阶子图</h4><p>我们称一个顶点 $v _ { i }$ 的 k 阶子图（k-subgraph）为：<br>$$<br>G _ { v _ { i } } ^ { ( k ) } = \left( V ^ { \prime } , E ^ { \prime } \right) , V ^ { \prime } = \left\{ v _ { j } | \forall v _ { j } , d \left( v _ { i } , v _ { j } \right) \leqslant k \right\} , E ^ { \prime } = \left\{ e _ { i j } | \forall v _ { j } , d \left( v _ { i } , v _ { j } \right) \leqslant k \right\}<br>$$<br>有时我们也称 k 阶子图为 k-hop，下图中的阴影部分就是顶点 $v _ { 1 }$ 的 2 阶子图：</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128103152.png" alt="图G的2阶子图"></p><h1 id="图的存储与遍历"><a class="header-anchor" href="#图的存储与遍历"></a>图的存储与遍历</h1><h2 id="邻接矩阵与关联矩阵"><a class="header-anchor" href="#邻接矩阵与关联矩阵"></a>邻接矩阵与关联矩阵</h2><h3 id="邻接矩阵"><a class="header-anchor" href="#邻接矩阵"></a>邻接矩阵</h3><p>设图 $G = ( V,\space E )$ ，在这里我们对边重新进行了编号 $e _ { 1 } , e _ { 2 } , \ldots , e _ { M }$，如下图所示：</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128105325.png" alt="图G示例"></p><p>我们用邻接矩阵 A 描述图中顶点之间的关联，$A \in R ^ { N \times N }$，其定义为：<br>$$<br>A _ { i j } = \left\{ \begin{array} { l } { 1 \text { if } \left( v _ { i } , v _ { j } \right) \subseteq E } \ { 0 \text { else } } \end{array} \right.<br>$$<br>用邻接矩阵（Adjacency matrix）存储图的时候，我们需要一个一维数组表示顶点集合，需要一个二维数组表示邻接矩阵。</p><p>需要特别说明的是，由于在实际的图数据中，邻接矩阵往往会出现大量的 0 值，因此可以用<strong>稀疏矩阵</strong>的格式来存储邻接矩阵，这样可以将邻接矩阵的空间复杂度控制在 $\mathrm { O } ( \mathrm { M } )$ 的范围内。</p><p>图 $G$ 的邻接矩阵存储表示：</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128110458.png" alt="邻接矩阵"></p><p>通过该图可以看出，无向图的邻接矩阵是沿主对角线对称的，即 $A _ { i j } = A _ { j i }$。</p><h3 id="关联矩阵"><a class="header-anchor" href="#关联矩阵"></a>关联矩阵</h3><p>除了邻接矩阵外，我们有时也用关联矩阵 $\mathrm { B } \in \mathrm { R } ^ { \mathrm { N } \times \mathrm { M } }$ 来描述节点与边之间的关联，定义如下：<br>$$<br>B _ { i j } = \left\{ \begin{array} { l } { 1 \text { if } v _ { i } \text { 与 } e _ { j } \text { 相连 } } \ { 0 \text { else } } \end{array} \right.<br>$$<br>同样，关联矩阵（Incidence matrix）也可以用稀疏矩阵来存储，这是因为B的任意一列仅有两个非 0 值。</p><p>图 $G$ 的关联矩阵存储表示：</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128111838.png" alt="关联矩阵"></p><h2 id="图的遍历"><a class="header-anchor" href="#图的遍历"></a>图的遍历</h2><p>图的遍历是指从图中的某一顶点出发，按照某种搜索算法沿着图中的边对图中的所有顶点访问一次且仅访问一次。</p><p>图的遍历主要有两种算法：深度优先搜索（DFS，Depth-First-Search）和广度优先搜索（BFS，Breadth-First-Search）。</p><h3 id="深度优先搜索"><a class="header-anchor" href="#深度优先搜索"></a>深度优先搜索</h3><p>深度优先搜索是一个递归算法，有回退过程。</p><p>其算法思想是：由图中某一顶点 $v _ { i }$ 出发，访问它的任意一个邻居 $w _ { 1 }$；再从 $w _ { 1 }$ 出发，访问 $w _ { 1 }$ 的所有邻居中未被访问过的顶点 $w _ { 2 }$；然后再从 $w _ { 2 }$ 出发，依次访问，直到出现某顶点不再有邻居未被访问过。接着，回退一步，回退到前一次刚访问过的顶点，看是否还有其他未被访问过的邻居，如果有，则访问该邻居，之后再从该邻居出发，进行与前面类似的访问；如果没有，就再回退一步进行类似访问。重复上述过程，直到该图中所有顶点都被访问过为止。</p><h3 id="广度优先搜索"><a class="header-anchor" href="#广度优先搜索"></a>广度优先搜索</h3><p>广度优先搜索是一个分层的搜索过程，没有回退过程。</p><p>其算法思想是：从图中某一顶点 $v _ { i }$ 开始，由 $v _ { i }$ 出发，依次访问 $v _ { i }$ 的所有未被访问过的邻居 $w _ { 1 } , w _ { 2 } , \ldots , w _ { n }$；然后再顺序访问 $w _ { 1 } , w _ { 2 } , \ldots , w _ { n }$ 的所有还未被访问过的邻居，如此一层层执行下去，直到图中所有顶点都被访问到为止。</p><h1 id="图数据的应用场景"><a class="header-anchor" href="#图数据的应用场景"></a>图数据的应用场景</h1><p>在实际的数据场景中，我们通常将图称为网络（Network），与之对应的，图的两个要素（顶点和边）也被称为节点（Node）和关系（Link），比如我们熟知的社交网络、物流网络等概念名词。</p><h2 id="图数据的类型"><a class="header-anchor" href="#图数据的类型"></a>图数据的类型</h2><h3 id="同构图"><a class="header-anchor" href="#同构图"></a>同构图</h3><p>同构图（Homogeneous Graph）是指图中的节点类型和关系类型都仅有一种。</p><p>同构图是实际图数据的一种最简化的情况，如由超链接关系所构成的万维网，这类图数据的信息全部包含在邻接矩阵里。</p><h3 id="异构图"><a class="header-anchor" href="#异构图"></a>异构图</h3><p>与同构图相反，异构图（Heterogeneous Graph）是指图中的节点类型或关系类型多于一种。</p><p>在现实场景中，我们通常研究的图数据对象是多类型的，对象之间的交互关系也是多样化的。因此，异构图能够更好地贴近现实。</p><h3 id="属性图"><a class="header-anchor" href="#属性图"></a>属性图</h3><p>相较于异构图，属性图（Property Graph）给图数据增加了额外的属性信息，如图所示：</p><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128171828.png" alt="属性图"></p><p>对于一个属性图而言，节点和关系都有标签（Label）和属性（Property），这里的标签是指节点或关系的类型，如某节点的类型为“用户”，属性是节点或关系的附加描述信息，如“用户”节点可以有“姓名”“注册时间”“注册地址”等属性。</p><p>属性图是一种最常见的工业级图数据的表示方式，能够广泛适用于多种业务场景下的数据表达。</p><h3 id="非显式图"><a class="header-anchor" href="#非显式图"></a>非显式图</h3><p>非显式图（Graph Constructed from Non-relational Data）是指数据之间没有显式地定义出关系，需要依据某种规则或计算方式将数据的关系表达出来，进而将数据当成一种图数据进行研究。</p><p>比如计算机3D视觉中的点云数据，如果我们将节点之间的空间距离转化成关系的话，点云数据就成了图数据。</p><h2 id="图数据应用示例"><a class="header-anchor" href="#图数据应用示例"></a>图数据应用示例</h2><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128172817.png" alt="图数据应用示例"></p><h3 id="社交网络"><a class="header-anchor" href="#社交网络"></a>社交网络</h3><p>社交网络是十分常见的一类图数据，代表着各种个人或组织之间的社会关系。</p><p>上图中的 a 图展示了在线社交网络中的用户关注网络：以用户为节点，用户之间的关注关系作为边。</p><p>这是一个典型的同构图，一般用来研究用户的重要性排名以及相关的用户推荐等问题。随着移动互联网技术的不断深入，更多元化的媒体对象被补充进社交网络中，比如短文本、视频等，如此构成的异构图可以完成更加多样化的任务。</p><h3 id="电子购物"><a class="header-anchor" href="#电子购物"></a>电子购物</h3><p>电子购物是互联网中的一类核心业务，在这类场景中，业务数据通常可以用一个用户–商品的二部图来描述。</p><p>在上图中的 b 图所展示的例子中，节点分为两类：用户和商品，存在的关系有浏览、收藏、购买等。用户与商品之间可以存在多重关系，如既存在收藏关系也存在购买关系。这类复杂的数据场景可以用属性图轻松描述。</p><p>电子购物催生了一项大家熟知的技术应用—推荐系统。用户与商品之间的交互关系，反映了用户的购物偏好。例如，经典的啤酒与尿布的故事：爱买啤酒的人通常也更爱买尿布。</p><h3 id="化学分子"><a class="header-anchor" href="#化学分子"></a>化学分子</h3><p>以原子为节点，原子之间的化学键作为边，我们可以将分子视为一种图数据进行研究，分子的基本构成以及内在联系决定了分子的各项理化性质，通常我们用其指导新材料、新药物的研究任务，如上图中的 c 图所示。</p><h3 id="交通网络"><a class="header-anchor" href="#交通网络"></a>交通网络</h3><p>交通网络具有多种形式，比如地铁网络中将各个站点作为节点，站点之间的连通性作为边构成一张图，如上图中的 d 图所示。</p><p>通常在交通网络中我们比较关注的是路径规划相关的问题：比如最短路径问题，再如我们将车流量作为网络中节点的属性，去预测未来交通流量的变化情况。</p><h3 id="场景图"><a class="header-anchor" href="#场景图"></a>场景图</h3><p>场景图是图像语义的一种描述方式，它将图像中的物体当作节点，物体之间的相互关系当作边构成一张图。场景图可以将关系复杂的图像简化成一个关系明确的语义图。</p><p>场景图具有十分强大的应用场景，如图像合成、图像语义检索、视觉推理等。上图中的 e 图为由场景图合成相关语义图像的示例，在该场景图中，描述了 5 个对象：两个男人、一个小孩、飞盘、庭院以及他们之间的关系，可以看到场景图具有很强的语义表示能力。</p><h3 id="电路设计图"><a class="header-anchor" href="#电路设计图"></a>电路设计图</h3><p>我们可以将电子器件如谐振器作为节点，器件之间的布线作为边将电路设计抽象成一种图数据。</p><p>在参考文献<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUNpcmN1aXQtR05OJUVGJUJDJTlBR3JhcGgrTmV1cmFsK05ldHdvcmtzK2ZvcitEaXN0cmlidXRlZCtDaXJjdWl0K0Rlc2lnbiZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Circuit-GNN%EF%BC%9AGraph+Neural+Networks+for+Distributed+Circuit+Design&amp;btnG=">[1]<i class="fa fa-external-link"></i></span>中，对电路设计进行了这样的抽象，如上图中的 f 图所示，然后基于图神经网络技术对电路的电磁特性进行仿真拟合，相较于严格的电磁学公式仿真，可以在可接受的误差范围内极大地加速高频电路的设计工作。</p><h1 id="图数据深度学习"><a class="header-anchor" href="#图数据深度学习"></a>图数据深度学习</h1><h2 id="基于图数据相关的任务学习理论"><a class="header-anchor" href="#基于图数据相关的任务学习理论"></a>基于图数据相关的任务学习理论</h2><h3 id="谱图理论"><a class="header-anchor" href="#谱图理论"></a>谱图理论</h3><p>谱图理论（Spectral Graph Theory）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNwZWN0cmFsK0dyYXBoK1RoZW9yeStBbWVyaWNhbitNYXRoZW1hdGljYWwrU29jaWV0eSZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Spectral+Graph+Theory+American+Mathematical+Society&amp;btnG=">[2]<i class="fa fa-external-link"></i></span>是将图论与线性代数相结合的理论，基于此理论发展而来的谱聚类相关算法<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErdHV0b3JpYWwrb24rc3BlY3RyYWwrY2x1c3RlcmluZyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+tutorial+on+spectral+clustering&amp;btnG=">[3]<i class="fa fa-external-link"></i></span>，可以用来解决图的分割或者节点的聚类问题。</p><h3 id="统计关系学习"><a class="header-anchor" href="#统计关系学习"></a>统计关系学习</h3><p>统计关系学习（Statistical Relational Learning）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUludHJvZHVjdGlvbit0bytzdGF0aXN0aWNhbCtyZWxhdGlvbmFsK2xlYXJuaW5nJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Introduction+to+statistical+relational+learning&amp;btnG=">[4]<i class="fa fa-external-link"></i></span>是将关系表示与似然表示相结合的机器学习理论。</p><p>区别于传统的机器学习算法对数据独立同分布（independent and Identically Distributed，数据对象是同类且独立不相关的）的假设，统计关系学习打破了对数据的上述两种假设，对图数据的学习具有更好的契合度。</p><h3 id="异构信息网络"><a class="header-anchor" href="#异构信息网络"></a>异构信息网络</h3><p>为了更加贴合实际场景中的异构图数据，异构信息网络（Heterogeneous Information Network）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErc3VydmV5K29mK2hldGVyb2dlbmVvdXMraW5mb3JtYXRpb24rbmV0d29yaythbmFseXNpcyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+survey+of+heterogeneous+information+network+analysis&amp;btnG=">[5]<i class="fa fa-external-link"></i></span>分析被提出，用以挖掘异构图中更加全面的结构信息和丰富的语义信息。</p><h3 id="网络表示学习"><a class="header-anchor" href="#网络表示学习"></a>网络表示学习</h3><p>由于这些年深度学习在实际应用领域取得的巨大成就，表示学习和端对端学习的概念日益得到重视，为了从复杂的图数据中学习到包含充分信息的向量化表示，出现了大量网络表示学习（Network Embedding）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErc3VydmV5K29uK25ldHdvcmsrZW1iZWRkaW5nJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+survey+on+network+embedding&amp;btnG=">[6]<i class="fa fa-external-link"></i></span>的方法。</p><p>然而网络表示学习很难提供表示学习加任务学习的端对端系统，基于此，图数据的端对端学习系统仍然是一个重要的研究课题。</p><h2 id="与图数据相对应的数据"><a class="header-anchor" href="#与图数据相对应的数据"></a>与图数据相对应的数据</h2><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200128220741.png" alt="图像和语音文本数据类型"></p><p>与图数据相对应的数据有图像、语音与文本，这些数据是定义在欧式空间中的规则化结构数据，基于这些数据的张量计算体系是比较自然且高效的。</p><h3 id="图像数据"><a class="header-anchor" href="#图像数据"></a>图像数据</h3><p>图像数据呈现出规则的 2D 栅格结构，这种栅格结构与卷积神经网络的作用机制具有良好的对应。</p><h3 id="语音数据"><a class="header-anchor" href="#语音数据"></a><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNDcwMzI2OA==" title="https://zhuanlan.zhihu.com/p/24703268">语音数据<i class="fa fa-external-link"></i></span></h3><p>声波是二维的，它在每个时刻都有一个基于其高度的值。为了将这个声波转换成数字，我们只记录声波在等距点的高度，这就是采样（sampling）。可以使用循环神经网络处理语音数据，因为它预测的每个字母都应该能够影响它对下一个字母的预测。</p><h3 id="文本数据"><a class="header-anchor" href="#文本数据"></a>文本数据</h3><p>文本数据是一种规则的序列数据，这种序列结构与循环神经网络的作用机制相对应。</p><h2 id="基于图卷积操作的神经网络理论"><a class="header-anchor" href="#基于图卷积操作的神经网络理论"></a>基于图卷积操作的神经网络理论</h2><p>受图信号处理（Graph Signal Processing）<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVRoZStlbWVyZ2luZytmaWVsZCtvZitzaWduYWwrcHJvY2Vzc2luZytvbitncmFwaHMlRUYlQkMlOUFFeHRlbmRpbmcraGlnaC1kaW1lbnNpb25hbCtkYXRhK2FuYWx5c2lzK3RvK25ldHdvcmtzK2FuZCtvdGhlcitpcnJlZ3VsYXIrZG9tYWlucyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=The+emerging+field+of+signal+processing+on+graphs%EF%BC%9AExtending+high-dimensional+data+analysis+to+networks+and+other+irregular+domains&amp;btnG=">[7]<i class="fa fa-external-link"></i></span>中对图信号卷积滤波的定义的启发，近几年发展出了一套基于图卷积操作并不断衍生的神经网络理论。<span class="exturl" data-url="aHR0cHM6Ly9pdGVtLmpkLmNvbS8xMjYxNTA2NS5odG1s" title="https://item.jd.com/12615065.html">本书<i class="fa fa-external-link"></i></span>将这类方法统称为图神经网络（Graph Neural Network，GNN<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUdyYXBoK25ldXJhbCtuZXR3b3JrcyVFRiVCQyU5QUErcmV2aWV3K29mK21ldGhvZHMrYW5kK2FwcGxpY2F0aW9ucyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Graph+neural+networks%EF%BC%9AA+review+of+methods+and+applications&amp;btnG=">[8]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPURlZXArbGVhcm5pbmcrb24rZ3JhcGhzJUVGJUJDJTlBQStzdXJ2ZXkmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Deep+learning+on+graphs%EF%BC%9AA+survey&amp;btnG=">[9]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErY29tcHJlaGVuc2l2ZStzdXJ2ZXkrb24rZ3JhcGgrbmV1cmFsK25ldHdvcmtzJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+comprehensive+survey+on+graph+neural+networks&amp;btnG=">[10]<i class="fa fa-external-link"></i></span>）。</p><p>2005 年，Marco Gori 等人发表论文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUErbmV3K21vZGVsK2ZvcitsZWFybmluZytpbitncmFwaCtkb21haW5zJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=A+new+model+for+learning+in+graph+domains&amp;btnG=">[11]<i class="fa fa-external-link"></i></span>，首次提出了图神经网络的概念。在此之前，处理图数据的方法是在数据的预处理阶段将图转换为用一组向量表示。这种处理方法最大的问题就是图中的结构信息可能会丢失，并且得到的结果会严重依赖于对图的预处理。GNN 的提出，便是为了能够将学习过程直接架构于图数据之上。</p><p>随后，其在 2009 年的两篇论文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPU5ldXJhbCtuZXR3b3JrK2ZvcitncmFwaHMlRUYlQkMlOUFBK2NvbnRleHR1YWwrY29uc3RydWN0aXZlK2FwcHJvYWNoJmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Neural+network+for+graphs%EF%BC%9AA+contextual+constructive+approach&amp;btnG=">[12]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVRoZStncmFwaCtuZXVyYWwrbmV0d29yayttb2RlbCZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=The+graph+neural+network+model&amp;btnG=">[13]<i class="fa fa-external-link"></i></span>中又进一步阐述了图神经网络，并提出了一种监督学习的方法来训练 GNN。但是，早期的这些研究都是以迭代的方式，通过循环神经网络传播邻居信息，直到达到稳定的固定状态来学习节点的表示。这种计算方式消耗非常大，相关研究开始关注如何改进这种方法以减小计算量。</p><p>2012年前后，卷积神经网络开始在视觉领域取得令人瞩目的成绩，于是人们开始考虑如何将卷积应用到图神经网络中。2013 年 Bruna 等人首次将卷积引入图神经网络中，在引文<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNwZWN0cmFsK25ldHdvcmtzK2FuZCtsb2NhbGx5K2Nvbm5lY3RlZCtuZXR3b3JrcytvbitncmFwaHMmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Spectral+networks+and+locally+connected+networks+on+graphs&amp;btnG=">[14]<i class="fa fa-external-link"></i></span>中基于频域卷积操作的概念开发了一种图卷积网络模型，首次将可学习的卷积操作用于图数据之上。自此以后，不断有人提出改进、拓展这种基于频域图卷积的神经网络模型。但是基于频域卷积的方法在计算时需要同时处理整个图，并且需要承担矩阵分解时的很高的时间复杂度，这很难使学习系统扩展到大规模图数据的学习任务上去，所以基于空域的图卷积被提出并逐渐流行。</p><p>2016年，Kipf 等人<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPVNlbWktc3VwZXJ2aXNlZCtjbGFzc2lmaWNhdGlvbit3aXRoK2dyYXBoK2NvbnZvbHV0aW9uYWwrbmV0d29ya3MmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Semi-supervised+classification+with+graph+convolutional+networks&amp;btnG=">[15]<i class="fa fa-external-link"></i></span>将频域图卷积的定义进行简化，使得图卷积的操作能够在空域进行，这极大地提升了图卷积模型的计算效率，同时，得益于卷积滤波的高效性，图卷积模型在多项图数据相关的任务上取得了令人瞩目的成绩。</p><p>近几年，更多的基于空域图卷积的神经网络模型的变体<span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUluZHVjdGl2ZStyZXByZXNlbnRhdGlvbitsZWFybmluZytvbitsYXJnZStncmFwaHMmYW1wO2J0bkc9" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Inductive+representation+learning+on+large+graphs&amp;btnG=">[16]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPUdyYXBoK2F0dGVudGlvbituZXR3b3JrcyZhbXA7YnRuRz0=" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Graph+attention+networks&amp;btnG=">[17]<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20uaGsvc2Nob2xhcj9obD16aC1DTiZhbXA7YXNfc2R0PTAlMkM1JmFtcDtxPU5ldXJhbCttZXNzYWdlK3Bhc3NpbmcrZm9yK3F1YW50dW0rY2hlbWlzdHJ5JmFtcDtidG5HPQ==" title="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Neural+message+passing+for+quantum+chemistry&amp;btnG=">[18]<i class="fa fa-external-link"></i></span>被开发出来，我们将这类方法统称为 GNN。各种 GNN 模型的出现，大大加强了学习系统对各类图数据的适应性，这也为各种图数据的任务学习奠定了坚实的基础。</p><p>自此，图数据与深度学习有了第一次真正意义上的结合。GNN 的出现，实现了图数据的端对端学习方式，为图数据的诸多应用场景下的任务，提供了一个极具竞争力的学习方案。</p><h2 id="图数据相关任务的分类"><a class="header-anchor" href="#图数据相关任务的分类"></a>图数据相关任务的分类</h2><h3 id="节点层面的任务"><a class="header-anchor" href="#节点层面的任务"></a>节点层面的任务</h3><p>节点层面（Node Level）的任务主要包括分类任务和回归任务。这类任务虽然是对节点层面的性质进行预测，但是显然不应该将模型建立在一个个单独的节点上，节点的关系也需要考虑。节点层面的任务有很多，包括学术上使用较多的对论文引用网络中的论文节点进行分类，工业界在线社交网络中用户标签的分类、恶意账户检测等。</p><h3 id="边层面的任务"><a class="header-anchor" href="#边层面的任务"></a>边层面的任务</h3><p>边层面（Link Level）的任务主要包括边的分类和预测任务。边的分类是指对边的某种性质进行预测；边预测是指给定的两个节点之间是否会构成边。常见的应用场景比如在社交网络中，将用户作为节点，用户之间的关注关系建模为边，通过边预测实现社交用户的推荐。目前，边层面的任务主要集中在推荐业务中。</p><h3 id="图层面的任务"><a class="header-anchor" href="#图层面的任务"></a>图层面的任务</h3><p>图层面（Graph Level）的任务不依赖于某个节点或者某条边的属性，而是从图的整体结构出发，实现分类、表示和生成等任务。目前，图层面的任务主要应用在自然科学研究领域，比如对药物分子的分类、酶的分类等。</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>图论</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>学习数据挖掘的最佳路径是什么（笔记）</title>
    <url>/2020/01/23/%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E6%9C%80%E4%BD%B3%E8%B7%AF%E5%BE%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><h1 id="数据挖掘知识清单"><a class="header-anchor" href="#数据挖掘知识清单"></a>数据挖掘知识清单</h1><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200122202555.jpg" alt="数据挖掘知识清单"></p><h2 id="数据挖掘的基本流程"><a class="header-anchor" href="#数据挖掘的基本流程"></a>数据挖掘的基本流程</h2><ul><li>商业理解<ul><li>从商业的角度理解项目需求</li><li>对数据挖掘的目标进行定义</li></ul></li><li>数据理解<ul><li>收集部分数据</li><li>对数据进行探索（包括数据描述、数据质量验证等）</li></ul></li><li>数据准备<ul><li>开始收集数据</li><li>数据清洗、数据集成等</li></ul></li><li>模型建立<ul><li>选择和应用各种数据挖掘模型</li><li>进行优化从而得到更好的结果</li></ul></li><li>模型评估<ul><li>对模型进行评价</li><li>检测构建模型的每个步骤</li><li>确认模型是否实现了预定的商业目标</li></ul></li><li>上线发布<ul><li>将从数据中找到的“知识”转化为用户可以使用的方式</li><li>呈现的形式可以是报告或者是可重复的数据挖掘过程</li><li>后续的监控和维护对数据挖掘结果的日常运营很重要</li></ul></li></ul><h2 id="数据挖掘的十大算法"><a class="header-anchor" href="#数据挖掘的十大算法"></a>数据挖掘的十大算法</h2><ul><li>分类算法<ul><li>C4.5<ul><li>决策树算法</li><li>创造性地在决策树构造过程中就进行了剪枝</li><li>可以处理连续的属性和不完整的数据</li></ul></li><li>朴素贝叶斯（Naive Bayes）<ul><li>基于概率论的原理</li><li>求解在未知物体出现的条件下各个类别出现的概率</li><li>哪个最大，就认为这个未知物体属于哪个分类</li></ul></li><li>SVM<ul><li>支持向量机（Support Vector Machine）</li><li>在训练中建立超平面</li></ul></li><li>KNN<ul><li>K 最近邻算法（K-Nearest Neighbor）</li><li>K 近邻即每个样本都可以用它最接近的 K 个邻居来代表</li></ul></li><li>AdaBoost<ul><li>boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法</li><li>在训练中建立一个联合的分类模型</li><li>让多个弱的分类器组成一个强的分类器</li></ul></li><li>CART<ul><li>分类和回归树（Classification and Regression Trees）</li><li>构建了一棵分类树和一棵回归树</li><li>属于决策树学习方法</li></ul></li></ul></li><li>聚类算法<ul><li>K-Means<ul><li>K 个类别，每个类别设置“中心点”</li><li>计算新点与 K 个中心点的距离</li><li>距离哪个中心点近就划分为该类</li></ul></li><li>EM<ul><li>最大期望算法，用于求参数的最大似然估计</li><li>假设 A 和 B 未知，A 和 B 都可以通过对方的值得到自己的估值</li><li>可以给其中一方赋初值，得到另一方的估值，然后反过来求取，持续这个过程直到收敛为止</li></ul></li></ul></li><li>关联分析<ul><li>Apriori<ul><li>是一种挖掘关联规则（association rules）的算法</li><li>通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系</li><li>频繁项集指经常出现在一起的物品的集合</li><li>关联规则暗示着两种物品之间可能存在很强的关系</li><li>被广泛应用到商业挖掘和网络安全等领域中</li></ul></li></ul></li><li>连接分析<ul><li>PageRank<ul><li>当一个页面链出的页面越多，说明这个页面的“参考文献”越多</li><li>当一个页面被链入的频率越高，说明这个页面被引用的次数越高</li><li>基于这个原理可以得到网站的权重划分</li></ul></li></ul></li></ul><h2 id="数据挖掘的数学原理"><a class="header-anchor" href="#数据挖掘的数学原理"></a>数据挖掘的数学原理</h2><ul><li>概率论与数理统计<ul><li>条件概率</li><li>独立性</li><li>随机变量</li><li>多维随机变量</li></ul></li><li>线性代数<ul><li>特征值和特征向量</li><li>用特征向量近似代表物体的特征（大数据降维的基本思路）</li><li>基于矩阵的运算和理论<ul><li>PCA 方法</li><li>SVD 方法</li><li>MF 方法</li><li>NMF 方法</li></ul></li></ul></li><li>图论<ul><li>起源自社交网络的好友关系表达</li><li>人与人的关系，可以用图论上的两个节点来进行连接</li><li>节点的度可以理解为一个人的朋友数</li><li>图论对于网络结构的分析非常有效</li><li>图论在关系挖掘和图像分割中有重要的作用</li></ul></li><li>最优化方法<ul><li>相当于机器学习中自我学习的过程</li><li>当机器知道了目标，训练后与结果存在偏差就需要迭代调整，最优化就是这个调整的过程</li><li>一般来说，这个学习和迭代的过程是漫长、随机的</li><li>最优化方法的提出就是用更短的时间得到收敛，取得更好的效果</li></ul></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Data Analysis</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>极客时间</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析全景图及修炼指南（笔记）</title>
    <url>/2020/01/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A8%E6%99%AF%E5%9B%BE%E5%8F%8A%E4%BF%AE%E7%82%BC%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><h1 id="高效学习方法-MAS"><a class="header-anchor" href="#高效学习方法-MAS"></a>高效学习方法 MAS</h1><ul><li>Multi-Dimension：从多个角度去认识一个事物</li><li>Ask：不懂就问</li><li>Sharing：最好的学习就是分享</li></ul><h2 id="如何与数据分析建立多维度连接"><a class="header-anchor" href="#如何与数据分析建立多维度连接"></a>如何与数据分析建立多维度连接</h2><ul><li>掌握<strong>基础概念</strong></li><li>熟练使用<strong>工具</strong></li><li>借助<strong>题库</strong>练习</li></ul><h2 id="学习数据分析的核心"><a class="header-anchor" href="#学习数据分析的核心"></a>学习数据分析的核心</h2><ul><li>培养数据思维</li><li>掌握挖掘工具</li><li>熟练实践并积累经验</li></ul><h1 id="数据分析的三个重要组成部分"><a class="header-anchor" href="#数据分析的三个重要组成部分"></a>数据分析的三个重要组成部分</h1><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200120231928.png" alt="数据分析的三个重要组成部分"></p><h2 id="数据采集"><a class="header-anchor" href="#数据采集"></a>数据采集</h2><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200120232150.png" alt="数据采集"></p><p>数据采集是数据分析的原材料，因为任何分析都要有数据源</p><ul><li>数据源<ul><li>开源数据源</li><li>爬虫抓取</li><li>日志采集</li><li>传感器</li></ul></li><li>工具使用<ul><li>八爪鱼</li><li>火车采集器</li><li>搜集客</li></ul></li><li>爬虫编写（Python 利器）<ul><li>PhantomJS（已停止更新）<ul><li>PhantomJS 是一个基于 webkit 的 JavaScript API</li><li>它使用 QtWebKit 作为它核心浏览器的功能，使用 webkit 来编译解释执行 JavaScript 代码</li><li>任何你可以在基于 webkit 浏览器做的事情，它都能做到</li></ul></li><li>Scarp<ul><li>Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架</li><li>可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中</li><li>其最初是为了页面抓取所设计的，也可以用于获取 API 所返回的数据或者通用的网络爬虫</li></ul></li><li>lxml<ul><li>lxml 是 python 的一个解析库，支持 HTML 和 XML 的解析，支持 XPath 解析方式，而且解析效率非常高</li><li>XPath，全称 XML Path Language，即 XML 路径语言，它是一门在 XML 文档中查找信息的语言，它最初是用来搜寻 XML 文档的，但是它同样适用于 HTML 文档的搜索</li></ul></li><li>Selenium<ul><li>Selenium 是一个用于 Web 应用程序测试的工具</li><li>Selenium 测试直接运行在浏览器中，就像真正的用户在操作一样</li><li>Selenium 是一套完整的 Web 应用程序测试系统，包含了测试的录制，编写及运行和测试的并行处理</li></ul></li></ul></li><li>实战<ul><li>如何自动抓取微博评论</li><li>如何自动下载明星海报</li><li>如何自动给微博加粉丝</li></ul></li></ul><h2 id="数据挖掘"><a class="header-anchor" href="#数据挖掘"></a>数据挖掘</h2><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200121011245.png" alt="数据挖掘"></p><p>之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。数据挖掘的核心是挖掘数据的商业价值，也就是我们所说的商业智能BI</p><ul><li>数学基础<ul><li>概率论与数理统计</li><li>线性代数</li><li>图论</li><li>最优化方法</li></ul></li><li>基本流程<ul><li>商业理解</li><li>数据理解</li><li>数据准备</li><li>模型建立</li><li>模型评估</li><li>上线发布</li></ul></li><li>十大算法<ul><li>分类算法<ul><li>C45</li><li>朴素贝叶斯</li><li>SVM</li><li>KNN</li><li>Adaboost</li><li>CART</li></ul></li><li>聚类算法<ul><li>K-Means</li><li>EM</li></ul></li><li>关联分析<ul><li>Apriori</li></ul></li><li>连接分析<ul><li>PageRank</li></ul></li></ul></li><li>实战<ul><li>如何对手写数字进行识别</li><li>如何进行乳腺癌检测</li><li>如何对文档进行归类</li></ul></li></ul><h2 id="数据可视化"><a class="header-anchor" href="#数据可视化"></a>数据可视化</h2><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200121105116.png" alt="数据可视化"></p><p>数据可视化可以让我们直观地了解到数据分析的结果</p><ul><li>工具使用<ul><li>微图</li><li>DataV</li><li>Data GIF Maker</li></ul></li><li>Python 可视化<ul><li>Matplotlib</li><li>Seaborn</li></ul></li></ul><h1 id="认知三部曲"><a class="header-anchor" href="#认知三部曲"></a>认知三部曲</h1><p><img data-src="https://blogpic-1253674047.cos.ap-chengdu.myqcloud.com/20200121110440.png" alt="认知三部曲"></p><blockquote><p>自上而下 疑问回答做分解</p></blockquote><p>带着问题找答案</p><blockquote><p>自下而上 概况总结做聚合</p></blockquote><p>带着概念作总结</p><h2 id="牢记原则"><a class="header-anchor" href="#牢记原则"></a>牢记原则</h2><ol><li>不重复造轮子</li><li>工具决定效率</li><li>熟练度（快速积累“资产”）</li></ol><h1 id="总结"><a class="header-anchor" href="#总结"></a>总结</h1><ol><li>记录一下每天的认知</li><li>这些认知对应工具的哪些操作</li><li>做更多练习来巩固你的认知</li></ol><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Data Analysis</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>极客时间</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN in Tensorflow</title>
    <url>/2019/05/14/CNN-in-Tensorflow/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><h1 id="TensorFlow-卷积层"><a class="header-anchor" href="#TensorFlow-卷积层"></a>TensorFlow 卷积层</h1><p>让我们看下如何在 TensorFlow 里面实现 CNN。</p><p>TensorFlow 提供了 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener"><code>tf.nn.conv2d()</code></a> 和 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/bias_add" target="_blank" rel="noopener"><code>tf.nn.bias_add()</code></a> 函数来创建你自己的卷积层。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Output depth</span></span><br><span class="line">k_output = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Image Properties</span></span><br><span class="line">image_width = <span class="number">10</span></span><br><span class="line">image_height = <span class="number">10</span></span><br><span class="line">color_channels = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convolution filter</span></span><br><span class="line">filter_size_width = <span class="number">5</span></span><br><span class="line">filter_size_height = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input/Image</span></span><br><span class="line">input = tf.placeholder(</span><br><span class="line">    tf.float32,</span><br><span class="line">    shape=[<span class="literal">None</span>, image_height, image_width, color_channels])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight and bias</span></span><br><span class="line"><span class="comment"># 由于卷积层的参数个数只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关，所以这里声明的参数变量是一个四维矩阵</span></span><br><span class="line"><span class="comment"># 前面两个维度代表了过滤器的尺寸，第三个维度表示当前层的深度，第四个维度表示过滤器的深度</span></span><br><span class="line">weight = tf.Variable(tf.truncated_normal(</span><br><span class="line">    [filter_size_height, filter_size_width, color_channels, k_output]))</span><br><span class="line">bias = tf.Variable(tf.zeros(k_output))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply Convolution</span></span><br><span class="line"><span class="comment"># 最后一个参数是填充（padding）的方法，TensorFlow 中提供 SAME 或是 VALID 两种选择</span></span><br><span class="line"><span class="comment"># 其中 SAME 表示添加全0填充，VALID 表示不填充</span></span><br><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># Add bias</span></span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br></pre></td></tr></table></figure><p>上述代码用了 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener"><code>tf.nn.conv2d()</code></a> 函数来计算卷积，<code>weights</code> 作为滤波器，<code>[1, 2, 2, 1]</code> 作为 strides（不同维度的步长）。TensorFlow 对每一个 <code>input</code> 维度使用一个单独的 stride 参数，<code>[batch, input_height, input_width, input_channels]</code>。我们通常把 <code>batch</code> 和 <code>input_channels</code> （<code>strides</code> 序列中的第一个第四个）的 stride 设为 <code>1</code>。</p><p><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/bias_add" target="_blank" rel="noopener"><code>tf.nn.bias_add()</code></a> 函数对矩阵的最后一维加了偏置项。</p><h2 id="卷积机制"><a class="header-anchor" href="#卷积机制"></a>卷积机制</h2><p>根据输入大小、滤波器大小，来决定输出维度（如下所示）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new_height &#x3D; (input_height - filter_height + 2 * P)&#x2F;S + 1</span><br><span class="line">new_width &#x3D; (input_width - filter_width + 2 * P)&#x2F;S + 1</span><br></pre></td></tr></table></figure><h1 id="TensorFlow-最大池化"><a class="header-anchor" href="#TensorFlow-最大池化"></a>TensorFlow 最大池化</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514143813.png" alt=""></p><p>这是一个最大池化的例子，<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29udm9sdXRpb25hbF9uZXVyYWxfbmV0d29yayNQb29saW5nX2xheWVy" title="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">max pooling<i class="fa fa-external-link"></i></span> 用了 2x2 的滤波器，stride 为 2。</p><p>例如 <code>[[1, 0], [4, 6]]</code> 生成 <code>6</code>，因为 <code>6</code> 是这4个数字中最大的。同理 <code>[[2, 3], [6, 8]]</code> 生成 <code>8</code>。<br>理论上，最大池化操作的好处是减小输入大小，使得神经网络能够专注于最重要的元素。最大池化只取覆盖区域中的最大值，其它的值都丢弃。</p><p>TensorFlow 提供了 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener"><code>tf.nn.max_pool()</code></a> 函数，用于对卷积层实现 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29udm9sdXRpb25hbF9uZXVyYWxfbmV0d29yayNQb29saW5nX2xheWVy" title="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">最大池化<i class="fa fa-external-link"></i></span> 。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br><span class="line"><span class="comment"># Apply Max Pooling</span></span><br><span class="line">conv_layer = tf.nn.max_pool(</span><br><span class="line">    conv_layer,</span><br><span class="line">    ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure><p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener"><code>tf.nn.max_pool()</code></a> 函数实现最大池化时， <code>ksize</code> 参数是滤波器大小，<code>strides</code> 参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。</p><p><code>ksize</code> 和 <code>strides</code> 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 (<code>[batch, height, width, channels]</code>)，对 <code>ksize</code> 和 <code>strides</code> 来说，batch 和 channel 通常都设置成 <code>1</code>。</p><h2 id="直观理解池化"><a class="header-anchor" href="#直观理解池化"></a>直观理解池化</h2><p>池化层总的来说是用来：<br>[ ] 增大输入大小<br>[x] 减小输出大小<br>[x] 避免过拟合<br>[ ] 获取更多信息</p><p>降低过拟合是减小输出大小的结果，它同样也减少了后续层中的参数的数量。</p><p>近期，池化层并不是很受青睐。部分原因是：</p><ul><li>现在的数据集又大又复杂，我们更关心欠拟合问题。</li><li>Dropout 是一个更好的正则化方法。</li><li>池化导致信息损失。想想最大池化的例子，<em>n</em> 个数字中我们只保留最大的，把余下的 <em>n-1</em> 完全舍弃了。</li></ul><h2 id="池化机制"><a class="header-anchor" href="#池化机制"></a>池化机制</h2><p><strong>设置</strong> H = height, W = width, D = depth</p><ul><li>输入维度是 4x4x5 (HxWxD)</li><li>滤波器大小 2x2 (HxW)</li><li>stride 的高和宽都是 2 (S)</li></ul><p>新的高和宽的公式是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new_height &#x3D; (input_height - filter_height)&#x2F;S + 1</span><br><span class="line">new_width &#x3D; (input_width - filter_width)&#x2F;S + 1</span><br></pre></td></tr></table></figure><p>注意：池化层的输出深度与输入的深度相同。另外池化操作是分别应用到每一个深度切片层。</p><h1 id="1x1-卷积"><a class="header-anchor" href="#1x1-卷积"></a>1x1 卷积</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514151303.png" alt=""></p><p>1x1 卷积关注的不是一块图像，而仅仅是一个像素。传统的卷积基本上是运行在一小块图像上的小分类器，但仅仅是个线性分类器，但如果你在中间加一个 1x1 卷积，你就用运行在一块图像上的神经网络代替了线性分类器。在卷积操作中散布一些 1x1 卷积是一种使模型变得更深的低耗高效的方法，并且会有更多的参数，但未完全改变神经网络结构。</p><h1 id="Inception-模块"><a class="header-anchor" href="#Inception-模块"></a>Inception 模块</h1><p>我们在卷积网络的每一层都可以选择进行池化运算、卷积运算，同时需要决定使用 1x1、3x3 还是 5x5 大小的卷积。其实这些对网络的建模能力都是有益的，所以我们可以将它们全部用上。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514152954.png" alt=""></p><p>Inception 模块不局限于单个卷积运算，而是将多个模块组合，例如平均池化后接 1x1 卷积、单独的 1x1 卷积、1x1 卷积接 3x3 卷积、1x1 卷积接 5x5 卷积，最后把这些运算的输出连成一串。根据选择参数的方式，模型中的参数总数可能非常少，但模型的性能比你使用简单卷积时要好。</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Tensorflow</tag>
        <tag>卷积</tag>
        <tag>池化</tag>
        <tag>Inception</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络简介</title>
    <url>/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><h1 id="神经网络"><a class="header-anchor" href="#神经网络"></a>神经网络</h1><p>神经网络是机器学习中的一个模型，可以用于两类问题的解答：</p><ul><li>分类：把数据划分成不同的类别</li><li>回归：建立数据间的连续关系</li></ul><h2 id="神经网络的分类问题"><a class="header-anchor" href="#神经网络的分类问题"></a>神经网络的分类问题</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505113856.png" alt="">例如我们是一所高校的招生人员，工作就是接收或拒绝申请的学生，评估方式为考试成绩和在校期间的平时成绩，通过以往的录取情况来预测新学生是否应该被接收。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114414.png" alt=""></p><p>我们该如何找到这条用于分类的直线呢？</p><h3 id="三维乃至更高维数据的情况"><a class="header-anchor" href="#三维乃至更高维数据的情况"></a>三维乃至更高维数据的情况</h3><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114931.png" alt=""></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505115040.png" alt=""></p><p>此时方程表示的是三维空间里的一个平面，或者是 n-1 维的超平面。</p><h1 id="感知器"><a class="header-anchor" href="#感知器"></a>感知器</h1><p><strong>感知器</strong>是神经网络的基础构成组件，表示方法通常如下：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505125428.png" alt=""></p><h2 id="用感知器实现逻辑运算-XOR（“异或”）"><a class="header-anchor" href="#用感知器实现逻辑运算-XOR（“异或”）"></a>用感知器实现逻辑运算 - XOR（“异或”）</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505142848.png" alt=""></p><p>其中感知器 A 为 AND，C 为 NOT，B 为 OR，通过上面的多层感知器可以计算出 XOR。</p><h2 id="感知器技巧-计算机如何“学习”分类？"><a class="header-anchor" href="#感知器技巧-计算机如何“学习”分类？"></a>感知器技巧 - 计算机如何“学习”分类？</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505143745.png" alt=""></p><p>整个数据集中的每一个点都会把分类的结果提供给感知器（分类函数），并调整感知器，例如被误分类的点希望直线更加靠近它们。——这就是计算机在神经网络算法中，找寻最优感知器的原理。</p><h2 id="感知器算法"><a class="header-anchor" href="#感知器算法"></a>感知器算法</h2><p>感知器步骤如下所示。对于坐标轴为 $(p,q)$ 的点，标签 y，以及等式 $\hat{y} = step(w_1x_1 + w_2x_2 + b)$ 给出的预测（其中 $\alpha$ 为学习率）：</p><ul><li>如果点分类正确，则什么也不做。</li><li>如果点分类为正，但是标签为负，则分别减去 $\alpha p$, $\alpha q$, 和 $\alpha$ 至 $w_1$, $w_2$, 和 $b$。</li><li>如果点分类为负，但是标签为正，则分别将 $\alpha p$, $\alpha q$, 和 $\alpha$ 加到 $w_1$, $w_2$, 和 $b$ 上。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptronStep</span><span class="params">(X, y, W, b, learn_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        y_hat = prediction(X[i], W, b)</span><br><span class="line">        <span class="keyword">if</span> y[i] - y_hat == <span class="number">1</span>:</span><br><span class="line">            W[<span class="number">0</span>] += learn_rate * X[i][<span class="number">0</span>]</span><br><span class="line">            W[<span class="number">1</span>] += learn_rate * X[i][<span class="number">1</span>]</span><br><span class="line">            b += learn_rate</span><br><span class="line">        <span class="keyword">elif</span> y[i] - y_hat == <span class="number">-1</span>:</span><br><span class="line">            W[<span class="number">0</span>] -= learn_rate * X[i][<span class="number">0</span>]</span><br><span class="line">            W[<span class="number">1</span>] -= learn_rate * X[i][<span class="number">1</span>]</span><br><span class="line">            b -= learn_rate</span><br><span class="line">    <span class="keyword">return</span> W, b</span><br></pre></td></tr></table></figure><h1 id="误差函数"><a class="header-anchor" href="#误差函数"></a>误差函数</h1><p>刚刚的感知器算法实现告诉我们，获取正确分类的方式，就是通过每一个错误分类的点，评估错误点位置与我们期望位置之间的差异，来慢慢的修正我们分类函数。</p><p>因为误差暗示了如何进行正确的分类，因此误差的定义就变得尤为重要，这也被称为<strong>误差函数</strong>，误差函数可以告诉我们当前与正确答案之间的差别有多大。</p><h2 id="误差函数与梯度下降"><a class="header-anchor" href="#误差函数与梯度下降"></a>误差函数与梯度下降</h2><p>误差函数提供给我们的预测值与实际值之间的差异，但是这个差异如何指导我们权重的更新呢？我们的目标是找到<strong>最小</strong>的误差函数值来找到与实际值误差最小的预测值。</p><p>假设一维问题是一条直线，那么二维问题就是一个平面，而三维问题就是一个曲面。曲面可以理解为有山峰也有低谷的地面，误差最小的地方就是低谷处，我们希望计算机找到的就是这个低谷的值。为了找到这个低谷，学者们发明了<strong>梯度下降</strong>。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505152716.png" alt=""></p><p>为了进行梯度下降，我们的误差函数不能是离散的，而必须是连续的。误差之巅的高度是连续函数，因为位置上的轻微扰动会导致高度发生变化，实际上误差函数必须是<strong>可微分</strong>的。</p><h2 id="Sigmoid-函数"><a class="header-anchor" href="#Sigmoid-函数"></a>Sigmoid 函数</h2><p>对于优化而言，连续型误差函数比离散型函数更好。为此，我们需要从离散型预测变成连续型预测，方法就是使用 Sigmoid 函数取代阶跃函数作为激活函数。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505155839.png" alt=""></p><p>Sigmoid 函数是一个在生物学中常见的 S 型函数，也称为 S 型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid 函数常被用作神经网络的阈值函数，将变量映射到 0,1 之间。</p><p>Sigmoid 函数由下列公式定义：<br>$$<br>\sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } }<br>$$<br>其对 $x$ 的导数可以用自身表示：<br>$$<br>\sigma ^ { \prime } ( x ) = \frac { e ^ { - x } } { \left( 1 + e ^ { - x } \right) ^ { 2 } } = \sigma ( x ) ( 1 - \sigma ( x ) )<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure><h2 id="Softmax-函数"><a class="header-anchor" href="#Softmax-函数"></a>Softmax 函数</h2><p>Softmax 函数和 S 型函数是对等的，但是问题具有 3 个或更多个类别。</p><p>Softmax 公式：<br>$$<br>S _ { i } = \frac { e ^ { i } } { \sum _ { j } e ^ { j } }<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(L)</span>:</span></span><br><span class="line">    expL = np.exp(L)</span><br><span class="line">    <span class="keyword">return</span> np.divide (expL, expL.sum())</span><br></pre></td></tr></table></figure><h2 id="One-hot-Encoding"><a class="header-anchor" href="#One-hot-Encoding"></a>One-hot Encoding</h2><p>计算机在表示多结果的分类时，使用 One-Hot 编码是比较常见的处理方式。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505163535.png" alt=""></p><h2 id="最大似然法"><a class="header-anchor" href="#最大似然法"></a>最大似然法</h2><p>最佳模型更有可能是对于实际在我们身上发生情况所对应的<strong>概率更大的模型</strong>，无论实际情况是被录取还是被拒，该方法叫做<strong>最大似然法（Maximum Likelihood）</strong>。</p><p>我们要做的是选出实际情况对应概率最大的模型，也就是说，通过<strong>最大化概率</strong>，我们可以选出最优的模型。</p><p>最大化概率是否就等价于最小化误差函数？也许的确是这样的。</p><h2 id="交叉熵"><a class="header-anchor" href="#交叉熵"></a>交叉熵</h2><p>概率和误差函数之间肯定有一定的联系，这种联系叫做<strong>交叉熵</strong>。这个概念在很多领域都非常流行，包括机器学习领域。</p><p>交叉熵具有以下特性：如果有一系列的事件及其对应的发生概率，如果事件发生的可能性大，则交叉熵较小；如果可能性小，那么交叉熵就会很大。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506093541.png" alt=""></p><p>交叉熵公式：<br>$$<br>CrossEntropy = - \sum _ { i = 1 } ^ { m } y_i \ln (p_i) + ( 1 - y_i ) \ln ( 1 - p_i )<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(Y, P)</span>:</span></span><br><span class="line">    Y = np.float_(Y)</span><br><span class="line">    P = np.float_(P)</span><br><span class="line">    <span class="keyword">return</span> -np.sum(Y * np.log(P) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - P))</span><br></pre></td></tr></table></figure><p>多类别交叉熵公式：<br>$$<br>CrossEntropy = - \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { m } y _ { i j } \ln \left( p _ { i j } \right)<br>$$</p><h1 id="梯度下降"><a class="header-anchor" href="#梯度下降"></a>梯度下降</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506104821.png" alt=""></p><p>上图中的梯度 $\nabla E$ 实际告诉了我们误差函数增长最快的方向，因此，如果沿着该梯度的反方向，将得到误差函数降低最快的方向。<br>$$<br>y = \sigma ( W x + b ) ← \text{Bad}<br>$$</p><p>$$<br>y = \sigma \left( w _ { 1 } x _ { 1 } + \ldots + w _ { n } x _ { n } + b \right)<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_formula</span><span class="params">(features, weights, bias)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(np.dot(features, weights) + bias)</span><br></pre></td></tr></table></figure><p>$$<br>\nabla E = \left( \partial E / \partial w _ { 1 } , \ldots , \partial E / \partial w _ { n } , \partial E / \partial b \right)<br>$$</p><p>误差函数的梯度就是误差函数相对权重和偏差的偏导数所组成的矢量，我们按照以下方式更新权重和偏差（$\alpha$ 为学习率，可取为 0.1）：<br>$$<br>w _ { i } ^ { \prime } ← w _ { i } - \alpha \frac {\partial E} {\partial w _ { i }}<br>$$</p><p>$$<br>b ^ { \prime } ← b - \alpha \frac {\partial E} {\partial b}<br>$$</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(x, y, weights, bias, learnrate)</span>:</span></span><br><span class="line">    output = output_formula(x, weights, bias)</span><br><span class="line">    d_error = y - output</span><br><span class="line">    weights += learnrate * d_error * x</span><br><span class="line">    bias += learnrate * d_error</span><br><span class="line">    <span class="keyword">return</span> weights, bias</span><br></pre></td></tr></table></figure><p>$$<br>y = \sigma \left( W ^ { \prime } x + b ^ { \prime } \right) ← \text{Better}<br>$$</p><h2 id="梯度计算"><a class="header-anchor" href="#梯度计算"></a>梯度计算</h2><p>Sigmoid 函数导数的计算：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506115945.png" alt=""></p><p>现在，如果有 $m$ 个样本点，标为 $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$, 误差公式是：<br>$$<br>E = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } \ln \left( \hat{y}^{(i)} \right) + \left( 1 - y ^ { ( i ) } \right) \ln \left( 1 - \hat{y}^{(i)} \right) \right)<br>$$<br>预测是 $\hat{y}^{(i)} = \sigma(Wx^{(i)} + b)$，我们的目标是计算 $E$, 在单个样本点 $x$ 时的梯度（偏导数），其中 $x$ 包含 n 个特征，即$x = \left( x _ { 1 } , \ldots , x _ { n } \right)$.<br>$$<br>\nabla E = \left( \frac { \partial } { \partial w _ { 1 } } E , \cdots , \frac { \partial } { \partial w _ { n } } E , \frac { \partial } { \partial b } E \right)<br>$$<br>为此，首先我们要计算$\frac { \partial } { \partial w _ { j } } \hat { y }$. 而 $\hat { y } = \sigma ( W x + b )$，因此：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506123529.png" alt=""></p><p>最后一个等式是因为和中的唯一非常量项相对于 $w_j$ 正好是 $w_j x_j$, 明显具有导数 $x_j$. 现在可以计算 $\frac { \partial } { \partial w _ { j } } E$：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506124100.png" alt=""></p><p>类似的计算将得出：<br>$$<br>\frac { \partial } { \partial b } E = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( y _ { i } - \hat { y } _ { i } \right)<br>$$<br>这个实际上告诉了我们很重要的规则。对于具有坐标 $(x_1, \ldots, x_n)$, 的点，标签 $y$, 预测 $\hat{y}$, 该点的误差函数梯度是 $\left(-(y - \hat{y})x_1, \cdots, -(y - \hat{y})x_n, -(y - \hat{y}) \right)$.</p><p>总之 $\nabla E ( W , b ) = - ( y - \hat { y } ) \left( x _ { 1 } , \ldots , x _ { n } , 1 \right)$.</p><p>如果思考下，会发现很神奇。**梯度实际上是标量乘以点的坐标！**什么是标量？也就是标签和预测之间的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。</p><p>请记下：<strong>小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。</strong></p><h2 id="感知器和梯度下降的区别"><a class="header-anchor" href="#感知器和梯度下降的区别"></a>感知器和梯度下降的区别</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506160811.png" alt=""></p><p>左右几乎是完全相同的，唯一的区别是，在左侧 $\hat{y}$ 可以是 0 到 1 之间的任何值，但是在右侧 $\hat{y}$ 只能是值 0 或 1。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506161953.png" alt=""></p><p>对于分类正确的点感知器算法什么都不会做，但是梯度下降算法会修改权重使得点更加远离直线。</p><h1 id="神经网络结构"><a class="header-anchor" href="#神经网络结构"></a>神经网络结构</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162439.png" alt=""></p><p>为了处理非线性数据，需要创建非线性模型，我们可以把两个线性模型组合成一个非线性模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162731.png" alt=""></p><p>现在的模型是两个之前的模型乘以权重，再加上某个偏差的线性组合。这就是构建神经网络的核心：对现有的线性模型进行线性组合，得到更复杂的新模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163008.png" alt=""></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163033.png" alt=""></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163051.png" alt=""></p><p>我们可以通过感知器的组合来构建神经网络，其中左侧的权重告诉我们线性模型具有的方程，而右侧的权重告诉我们两个模型的线性组合是多少，以便获得右侧的曲线非线性模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163649.png" alt=""></p><p>每当你看到左侧的神经网络，思考下该神经网络定义的非线性界线是什么。</p><h2 id="多层级"><a class="header-anchor" href="#多层级"></a>多层级</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163918.png" alt=""></p><p>神经网络通常可以划分为输入层、隐藏层和输出层。除此之外我们可以执行以下操作：</p><ul><li>向输入、隐藏和输出层添加更多节点。</li><li>添加更多层级。</li></ul><p>更多隐藏层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164131.png" alt=""></p><p>这让我们得到输出层中的三角形边界！</p><p>更多输入层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164240.png" alt=""></p><p>通常，如果输入层里有 n 个节点，那么处理的就是 n 维空间的数据。</p><p>更多输出层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506165731.png" alt=""></p><p>这只是表明我们有更多的输出，这就是具有多种类别的分类模型。</p><p>更多隐藏层的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507012253.png" alt=""></p><p>此时就形成了深度神经网络，现在这些线性模型相结合，形成非线性模型，这些非线性模型进一步再结合，形成更多的非线性模型。通常，我们可以这么操作很多次，形成具有大量隐藏层的复杂模型，这就是神经网络的神奇所在。</p><p>神经网络使用高度非线性化的边界拆分整个 n 维空间。</p><h2 id="前向反馈"><a class="header-anchor" href="#前向反馈"></a>前向反馈</h2><p>前向反馈是神经网络用来将输入变成输出的流程。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507104424.png" alt=""></p><p>对于含有两个隐藏层的神经网络来说公式如下：<br>$$<br>\hat { y } = \sigma \circ W ^ { ( 3 ) } \circ \sigma \circ W ^ { ( 2 ) } \circ \sigma \circ W ^ { ( 1 ) } ( x )<br>$$</p><h2 id="反向传播"><a class="header-anchor" href="#反向传播"></a>反向传播</h2><p>反向传播的流程：</p><ul><li>进行前向反馈运算。</li><li>将模型的输出与期望的输出进行比较。</li><li>计算误差。</li><li>向后运行前向反馈运算（反向传播），将误差分散到每个权重上。</li><li>更新权重，并获得更好的模型。</li><li>继续此流程，直到获得很好的模型。</li></ul><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507105250.png" alt=""></p><h3 id="链式法则"><a class="header-anchor" href="#链式法则"></a>链式法则</h3><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507122314.png" alt=""></p><p>简而言之，对复合函数求导，就是一系列导数的乘积。</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>感知机</tag>
        <tag>误差函数</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>NoSQL 和 SQL 的历史</title>
    <url>/2019/05/05/NoSQL-%E5%92%8C-SQL-%E7%9A%84%E5%8E%86%E5%8F%B2/</url>
    <content><![CDATA[<!-- build time:Wed Jan 29 2020 02:31:27 GMT+0800 (China Standard Time) --><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/201508281002.jpg" alt=""></p><h1 id="1970：我们没有-SQL"><a class="header-anchor" href="#1970：我们没有-SQL"></a>1970：我们没有 SQL</h1><p>最初程序员使用结构体或Class表示一辆车子，但是随着需求的增加，需要转换成用动态的&quot;映射&quot;来表示。</p><p>对 Java 程序员来说，映射就是一个 Map&lt;Object, Object&gt;。具体实现可以是 Hash 或者有序树，对应 HashMap 和 TreeMap。</p><h2 id="B-tree：Map-的持久化方式"><a class="header-anchor" href="#B-tree：Map-的持久化方式"></a>B-tree：Map 的持久化方式</h2><p>为了防止数据丢失，我们需要想办法把这个 Map 对象以非常高效的方式持久化下来，放到磁盘上。而 map.put/get 操作会有一次寻找的过程，这个寻找过程对于磁盘来说会转变为一次随机寻道过程。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20181201000011593.png" alt="B-tree"></p><p>上图是一个最简单的 B-tree，原理很简单：既然磁盘寻道时间很多，那就减少它，一次寻道能够从磁盘取更多数据就行了。所以它是以数组为单位存储数据的，数组满了以后 B-tree 会进行分裂，HBase 其实就是棵巨大的、分布式的 B-tree。</p><h2 id="层次数据库"><a class="header-anchor" href="#层次数据库"></a>层次数据库</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/201508281004.jpg" alt="层次模型"></p><p>层次模型如上图所示，可以用小汽车来表述：一个小车由四扇玻璃，四个轮子，两个反光镜组成。车有自己的属性，轮子有自己的属性，反光镜有自己的属性。</p><p>如果用 Java 代码来写的话，就是 Map 套 Map，每个 Map 有一些固定的属性，比如这个 Map 的名字是什么，这个 Map 的属性是什么, 而这就是我们最开始在使用的数据库了。</p><h1 id="1980：知道-SQL"><a class="header-anchor" href="#1980：知道-SQL"></a>1980：知道 SQL</h1><p>虽然关系数据库是上世纪 70 年代发明的，但是直到 80 年代，IBM 发布了第一代全功能的关系数据库系统 System R 后，我们才正式进入到关系数据库模型。</p><p>自从爱因斯坦把牛顿那由完美数学保证的自洽理论踢出了神坛，数学自洽就再也不是真理的标准了，哪个的用户最多哪个就是真理。</p><h2 id="关系模型易用在哪里"><a class="header-anchor" href="#关系模型易用在哪里"></a>关系模型易用在哪里</h2><p>如果我要做这样的一个查询：把厂里生产的所有汽车里面，左轮子供应商是 DRDS 的轮胎都找出来。</p><p>采用层次模型的代码是：</p><p><code>遍历每一辆车，从车对象中找到左面的轮子，查看轮子的属性，如果是 DRDS，留下，不是则丢弃。</code></p><p>采用关系模型的代码是：</p><p><code>select * from 轮子表 where 轮子位置='左' and 轮子供应商='DRDS'</code></p><h1 id="2000：No-SQL"><a class="header-anchor" href="#2000：No-SQL"></a>2000：No SQL!</h1><p>分布式事务性能太差，导致有一群人想要绕开这个问题。他们发现确实有一些场景是不需要强一致事务的，甚至连 SQL 都可以不要。最典型的就是日志流水的记录与分析这类场景。去掉了事务和 SQL，接口简单了，性能就更容易得到提升，扩展性也更容易实现，这就是NoSQL系统的起源。</p><h1 id="2005：-不仅仅是-SQL"><a class="header-anchor" href="#2005：-不仅仅是-SQL"></a>2005： 不仅仅是 SQL</h1><p>NoSQL 无法颠覆 SQL，双方共存。</p><h1 id="2013：No-SQL"><a class="header-anchor" href="#2013：No-SQL"></a>2013：No, SQL!</h1><p>经过了 10 年的折腾，人们还是发现关系模型目前来说是最方便表达数据存取的语言，比其他都要方便的多，所以还是妥协吧。于是所有的 NoSQL 都在想办法尝试支持关系数据库。</p><p>最终大家殊途同归，还是回到了如何能够让关系数据库更具有扩展性，性能更好这条路上来，这就是 NewSQL 的来源。</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>NoSQL</tag>
        <tag>B-tree</tag>
      </tags>
  </entry>
</search>
