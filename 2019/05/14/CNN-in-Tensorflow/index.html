<!-- build time:Wed Jan 29 2020 02:54:45 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="yandex-verification" content="3ac9ae36ddebb425"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://lsj210001.github.io").hostname,root:"/",scheme:"Gemini",version:"7.7.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!0},path:"search.xml",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="TensorFlow 卷积层让我们看下如何在 TensorFlow 里面实现 CNN。TensorFlow 提供了 tf.nn.conv2d() 和 tf.nn.bias_add() 函数来创建你自己的卷积层。# Output depthk_output &#x3D; 64# Image Propertiesimage_width &#x3D; 10image_height &#x3D; 10color_channels &#x3D;"><meta property="og:type" content="article"><meta property="og:title" content="CNN in Tensorflow"><meta property="og:url" content="https://lsj210001.github.io/2019/05/14/CNN-in-Tensorflow/index.html"><meta property="og:site_name" content="Morgan’s Blog"><meta property="og:description" content="TensorFlow 卷积层让我们看下如何在 TensorFlow 里面实现 CNN。TensorFlow 提供了 tf.nn.conv2d() 和 tf.nn.bias_add() 函数来创建你自己的卷积层。# Output depthk_output &#x3D; 64# Image Propertiesimage_width &#x3D; 10image_height &#x3D; 10color_channels &#x3D;"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514143813.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514151303.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514152954.png"><meta property="article:published_time" content="2019-05-14T13:45:09.000Z"><meta property="article:modified_time" content="2020-01-28T18:33:01.042Z"><meta property="article:author" content="Morgan"><meta property="article:tag" content="CNN"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="卷积"><meta property="article:tag" content="池化"><meta property="article:tag" content="Inception"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514143813.png"><link rel="canonical" href="https://lsj210001.github.io/2019/05/14/CNN-in-Tensorflow/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>CNN in Tensorflow | Morgan’s Blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?7680854d4de4d9d556876810e04cf1c8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Morgan’s Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-bullhorn"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">6</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">16</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">4</span></a></li><li class="menu-item menu-item-docs"><a href="/docs/" rel="section"><i class="fa fa-fw fa-book"></i>Docs</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0" title="Fork NexT on GitHub" aria-label="Fork NexT on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://lsj210001.github.io/2019/05/14/CNN-in-Tensorflow/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/apple-touch-icon-next.png"><meta itemprop="name" content="Morgan"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Morgan’s Blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">CNN in Tensorflow<span class="exturl post-edit-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmcvZWRpdC9zb3VyY2Uvc291cmNlL19wb3N0cy9DTk4taW4tVGVuc29yZmxvdy5tZA==" title="编辑"><i class="fa fa-pencil"></i></span></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-14 13:45:09" itemprop="dateCreated datePublished" datetime="2019-05-14T13:45:09Z">2019-05-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-01-28 18:33:01" itemprop="dateModified" datetime="2020-01-28T18:33:01Z">2020-01-28</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span> </a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus：</span> <a title="disqus" href="/2019/05/14/CNN-in-Tensorflow/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/14/CNN-in-Tensorflow/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>2.7k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="tensorflow-卷积层">TensorFlow 卷积层</h1><p>让我们看下如何在 TensorFlow 里面实现 CNN。</p><p>TensorFlow 提供了 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener"><code>tf.nn.conv2d()</code></a> 和 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/bias_add" target="_blank" rel="noopener"><code>tf.nn.bias_add()</code></a> 函数来创建你自己的卷积层。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Output depth</span></span><br><span class="line">k_output = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Image Properties</span></span><br><span class="line">image_width = <span class="number">10</span></span><br><span class="line">image_height = <span class="number">10</span></span><br><span class="line">color_channels = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convolution filter</span></span><br><span class="line">filter_size_width = <span class="number">5</span></span><br><span class="line">filter_size_height = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input/Image</span></span><br><span class="line">input = tf.placeholder(</span><br><span class="line">    tf.float32,</span><br><span class="line">    shape=[<span class="literal">None</span>, image_height, image_width, color_channels])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight and bias</span></span><br><span class="line"><span class="comment"># 由于卷积层的参数个数只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关，所以这里声明的参数变量是一个四维矩阵</span></span><br><span class="line"><span class="comment"># 前面两个维度代表了过滤器的尺寸，第三个维度表示当前层的深度，第四个维度表示过滤器的深度</span></span><br><span class="line">weight = tf.Variable(tf.truncated_normal(</span><br><span class="line">    [filter_size_height, filter_size_width, color_channels, k_output]))</span><br><span class="line">bias = tf.Variable(tf.zeros(k_output))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply Convolution</span></span><br><span class="line"><span class="comment"># 最后一个参数是填充（padding）的方法，TensorFlow 中提供 SAME 或是 VALID 两种选择</span></span><br><span class="line"><span class="comment"># 其中 SAME 表示添加全0填充，VALID 表示不填充</span></span><br><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># Add bias</span></span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line"><span class="comment"># Apply activation function</span></span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br></pre></td></tr></table></figure><p>上述代码用了 <a href="https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener"><code>tf.nn.conv2d()</code></a> 函数来计算卷积，<code>weights</code> 作为滤波器，<code>[1, 2, 2, 1]</code> 作为 strides（不同维度的步长）。TensorFlow 对每一个 <code>input</code> 维度使用一个单独的 stride 参数，<code>[batch, input_height, input_width, input_channels]</code>。我们通常把 <code>batch</code> 和 <code>input_channels</code> （<code>strides</code> 序列中的第一个第四个）的 stride 设为 <code>1</code>。</p><p><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/bias_add" target="_blank" rel="noopener"><code>tf.nn.bias_add()</code></a> 函数对矩阵的最后一维加了偏置项。</p><h2 id="卷积机制">卷积机制</h2><p>根据输入大小、滤波器大小，来决定输出维度（如下所示）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new_height &#x3D; (input_height - filter_height + 2 * P)&#x2F;S + 1</span><br><span class="line">new_width &#x3D; (input_width - filter_width + 2 * P)&#x2F;S + 1</span><br></pre></td></tr></table></figure><h1 id="tensorflow-最大池化">TensorFlow 最大池化</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514143813.png"></p><p>这是一个最大池化的例子，<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29udm9sdXRpb25hbF9uZXVyYWxfbmV0d29yayNQb29saW5nX2xheWVy" title="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">max pooling<i class="fa fa-external-link"></i></span> 用了 2x2 的滤波器，stride 为 2。</p><p>例如 <code>[[1, 0], [4, 6]]</code> 生成 <code>6</code>，因为 <code>6</code> 是这4个数字中最大的。同理 <code>[[2, 3], [6, 8]]</code> 生成 <code>8</code>。 理论上，最大池化操作的好处是减小输入大小，使得神经网络能够专注于最重要的元素。最大池化只取覆盖区域中的最大值，其它的值都丢弃。</p><p>TensorFlow 提供了 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener"><code>tf.nn.max_pool()</code></a> 函数，用于对卷积层实现 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29udm9sdXRpb25hbF9uZXVyYWxfbmV0d29yayNQb29saW5nX2xheWVy" title="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">最大池化<i class="fa fa-external-link"></i></span> 。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br><span class="line"><span class="comment"># Apply Max Pooling</span></span><br><span class="line">conv_layer = tf.nn.max_pool(</span><br><span class="line">    conv_layer,</span><br><span class="line">    ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure><p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener"><code>tf.nn.max_pool()</code></a> 函数实现最大池化时， <code>ksize</code> 参数是滤波器大小，<code>strides</code> 参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。</p><p><code>ksize</code> 和 <code>strides</code> 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 (<code>[batch, height, width, channels]</code>)，对 <code>ksize</code> 和 <code>strides</code> 来说，batch 和 channel 通常都设置成 <code>1</code>。</p><h2 id="直观理解池化">直观理解池化</h2><p>池化层总的来说是用来：</p><ul><li>[ ] 增大输入大小</li><li>[x] 减小输出大小</li><li>[x] 避免过拟合</li><li>[ ] 获取更多信息</li></ul><p>降低过拟合是减小输出大小的结果，它同样也减少了后续层中的参数的数量。</p><p>近期，池化层并不是很受青睐。部分原因是：</p><ul><li>现在的数据集又大又复杂，我们更关心欠拟合问题。</li><li>Dropout 是一个更好的正则化方法。</li><li>池化导致信息损失。想想最大池化的例子，<em>n</em> 个数字中我们只保留最大的，把余下的 <em>n-1</em> 完全舍弃了。</li></ul><h2 id="池化机制">池化机制</h2><p><strong>设置</strong> H = height, W = width, D = depth</p><ul><li>输入维度是 4x4x5 (HxWxD)</li><li>滤波器大小 2x2 (HxW)</li><li>stride 的高和宽都是 2 (S)</li></ul><p>新的高和宽的公式是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new_height &#x3D; (input_height - filter_height)&#x2F;S + 1</span><br><span class="line">new_width &#x3D; (input_width - filter_width)&#x2F;S + 1</span><br></pre></td></tr></table></figure><p>注意：池化层的输出深度与输入的深度相同。另外池化操作是分别应用到每一个深度切片层。</p><h1 id="x1-卷积">1x1 卷积</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514151303.png"></p><p>1x1 卷积关注的不是一块图像，而仅仅是一个像素。传统的卷积基本上是运行在一小块图像上的小分类器，但仅仅是个线性分类器，但如果你在中间加一个 1x1 卷积，你就用运行在一块图像上的神经网络代替了线性分类器。在卷积操作中散布一些 1x1 卷积是一种使模型变得更深的低耗高效的方法，并且会有更多的参数，但未完全改变神经网络结构。</p><h1 id="inception-模块">Inception 模块</h1><p>我们在卷积网络的每一层都可以选择进行池化运算、卷积运算，同时需要决定使用 1x1、3x3 还是 5x5 大小的卷积。其实这些对网络的建模能力都是有益的，所以我们可以将它们全部用上。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190514152954.png"></p><p>Inception 模块不局限于单个卷积运算，而是将多个模块组合，例如平均池化后接 1x1 卷积、单独的 1x1 卷积、1x1 卷积接 3x3 卷积、1x1 卷积接 5x5 卷积，最后把这些运算的输出连成一串。根据选择参数的方式，模型中的参数总数可能非常少，但模型的性能比你使用简单卷积时要好。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/CNN/" rel="tag"># CNN</a> <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a> <a href="/tags/%E5%8D%B7%E7%A7%AF/" rel="tag"># 卷积</a> <a href="/tags/%E6%B1%A0%E5%8C%96/" rel="tag"># 池化</a> <a href="/tags/Inception/" rel="tag"># Inception</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/" rel="prev" title="神经网络简介"><i class="fa fa-chevron-left"></i> 神经网络简介</a></div><div class="post-nav-item"><a href="/2020/01/21/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A8%E6%99%AF%E5%9B%BE%E5%8F%8A%E4%BF%AE%E7%82%BC%E6%8C%87%E5%8D%97%EF%BC%88%E7%AC%94%E8%AE%B0%EF%BC%89/" rel="next" title="数据分析全景图及修炼指南（笔记）">数据分析全景图及修炼指南（笔记） <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow-卷积层"><span class="nav-number">1.</span> <span class="nav-text">TensorFlow 卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积机制"><span class="nav-number">1.1.</span> <span class="nav-text">卷积机制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow-最大池化"><span class="nav-number">2.</span> <span class="nav-text">TensorFlow 最大池化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#直观理解池化"><span class="nav-number">2.1.</span> <span class="nav-text">直观理解池化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化机制"><span class="nav-number">2.2.</span> <span class="nav-text">池化机制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#x1-卷积"><span class="nav-number">3.</span> <span class="nav-text">1x1 卷积</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#inception-模块"><span class="nav-number">4.</span> <span class="nav-text">Inception 模块</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Morgan" src="/images/apple-touch-icon-next.png"><p class="site-author-name" itemprop="name">Morgan</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xzajIxMDAwMQ==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lsj210001"><i class="fa fa-fw fa-github"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOmR0Ymxsc2pAZ21haWwuY29t" title="E-Mail → mailto:dtbllsj@gmail.com"><i class="fa fa-fw fa-envelope"></i></span> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXR0ZXIuaW0vdGhlbWUtbmV4dA==" title="Gitter → https:&#x2F;&#x2F;gitter.im&#x2F;theme-next"><i class="fa fa-fw fa-github-alt"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9yaW90LmltL2FwcC8jL3Jvb20vI05leFQ6bWF0cml4Lm9yZw==" title="Riot → https:&#x2F;&#x2F;riot.im&#x2F;app&#x2F;#&#x2F;room&#x2F;#NexT:matrix.org"><i class="fa fa-fw fa-bullhorn"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL2pvaW5jaGF0L0dVTkhYQS12WmtnU011aW1MMVZtTXc=" title="Telegram [Chat] → https:&#x2F;&#x2F;t.me&#x2F;joinchat&#x2F;GUNHXA-vZkgSMuimL1VmMw"><i class="fa fa-fw fa-telegram"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL3RoZW1lX25leHQ=" title="Telegram [News] → https:&#x2F;&#x2F;t.me&#x2F;theme_next"><i class="fa fa-fw fa-paper-plane"></i></span></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span></div><div class="cc-license motion-element" itemprop="sponsor"><span class="exturl cc-opacity" title="Deploy with Netlify → https://www.netlify.com" data-url="aHR0cHM6Ly93d3cubmV0bGlmeS5jb20="><img width="80" src="https://www.netlify.com/img/global/badges/netlify-dark.svg" alt="Netlify"></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2014 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">NexT</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">19k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">18 分钟</span></div><div class="footer-custom">Theme source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0">here</span><br>Website source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmc=">here</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>function loadCount(){var t=document,d=t.createElement("script");d.src="https://theme-next.disqus.com/count.js",d.id="dsq-count-scr",(t.head||t.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://lsj210001.github.io/2019/05/14/CNN-in-Tensorflow/";
    this.page.identifier = "2019/05/14/CNN-in-Tensorflow/";
    this.page.title = "CNN in Tensorflow";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://theme-next.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html><!-- rebuild by neat -->