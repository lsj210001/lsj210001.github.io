<!-- build time:Wed Jan 29 2020 02:48:09 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="yandex-verification" content="3ac9ae36ddebb425"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("https://lsj210001.github.io").hostname,root:"/",scheme:"Gemini",version:"7.7.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!0},path:"search.xml",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="神经网络神经网络是机器学习中的一个模型，可以用于两类问题的解答：分类：把数据划分成不同的类别回归：建立数据间的连续关系神经网络的分类问题例如我们是一所高校的招生人员，工作就是接收或拒绝申请的学生，评估方式为考试成绩和在校期间的平时成绩，通过以往的录取情况来预测新学生是否应该被接收。我们该如何找到这条用于分类的直线呢？三维乃至更高维数据的情况此时方程表示的是三维空间里的一个平面，或者是 n-1 维的"><meta property="og:type" content="article"><meta property="og:title" content="神经网络简介"><meta property="og:url" content="https://lsj210001.github.io/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/index.html"><meta property="og:site_name" content="Morgan’s Blog"><meta property="og:description" content="神经网络神经网络是机器学习中的一个模型，可以用于两类问题的解答：分类：把数据划分成不同的类别回归：建立数据间的连续关系神经网络的分类问题例如我们是一所高校的招生人员，工作就是接收或拒绝申请的学生，评估方式为考试成绩和在校期间的平时成绩，通过以往的录取情况来预测新学生是否应该被接收。我们该如何找到这条用于分类的直线呢？三维乃至更高维数据的情况此时方程表示的是三维空间里的一个平面，或者是 n-1 维的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505113856.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114414.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114931.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505115040.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505125428.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505142848.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505143745.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505152716.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505155839.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505163535.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506093541.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506104821.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506115945.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506123529.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506124100.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506160811.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506161953.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162439.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162731.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163008.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163033.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163051.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163649.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163918.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164131.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164240.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506165731.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507012253.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507104424.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507105250.png"><meta property="og:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507122314.png"><meta property="article:published_time" content="2019-05-05T11:31:19.000Z"><meta property="article:modified_time" content="2019-05-07T17:57:19.282Z"><meta property="article:author" content="Morgan"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="感知机"><meta property="article:tag" content="误差函数"><meta property="article:tag" content="梯度下降"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505113856.png"><link rel="canonical" href="https://lsj210001.github.io/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>神经网络简介 | Morgan’s Blog</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?7680854d4de4d9d556876810e04cf1c8";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Morgan’s Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-bullhorn"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">6</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">16</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">4</span></a></li><li class="menu-item menu-item-docs"><a href="/docs/" rel="section"><i class="fa fa-fw fa-book"></i>Docs</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0" title="Fork NexT on GitHub" aria-label="Fork NexT on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://lsj210001.github.io/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/apple-touch-icon-next.png"><meta itemprop="name" content="Morgan"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Morgan’s Blog"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">神经网络简介<span class="exturl post-edit-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmcvZWRpdC9zb3VyY2Uvc291cmNlL19wb3N0cy/npZ7nu4/nvZHnu5znroDku4subWQ=" title="编辑"><i class="fa fa-pencil"></i></span></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-05 11:31:19" itemprop="dateCreated datePublished" datetime="2019-05-05T11:31:19Z">2019-05-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-05-07 17:57:19" itemprop="dateModified" datetime="2019-05-07T17:57:19Z">2019-05-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span> </a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus：</span> <a title="disqus" href="/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/05/神经网络简介/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.2k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="神经网络">神经网络</h1><p>神经网络是机器学习中的一个模型，可以用于两类问题的解答：</p><ul><li>分类：把数据划分成不同的类别</li><li>回归：建立数据间的连续关系</li></ul><h2 id="神经网络的分类问题">神经网络的分类问题</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505113856.png">例如我们是一所高校的招生人员，工作就是接收或拒绝申请的学生，评估方式为考试成绩和在校期间的平时成绩，通过以往的录取情况来预测新学生是否应该被接收。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114414.png"></p><p>我们该如何找到这条用于分类的直线呢？</p><h3 id="三维乃至更高维数据的情况">三维乃至更高维数据的情况</h3><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505114931.png"></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505115040.png"></p><p>此时方程表示的是三维空间里的一个平面，或者是 n-1 维的超平面。</p><h1 id="感知器">感知器</h1><p><strong>感知器</strong>是神经网络的基础构成组件，表示方法通常如下：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505125428.png"></p><h2 id="用感知器实现逻辑运算---xor异或">用感知器实现逻辑运算 - XOR（“异或”）</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505142848.png"></p><p>其中感知器 A 为 AND，C 为 NOT，B 为 OR，通过上面的多层感知器可以计算出 XOR。</p><h2 id="感知器技巧---计算机如何学习分类">感知器技巧 - 计算机如何“学习”分类？</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505143745.png"></p><p>整个数据集中的每一个点都会把分类的结果提供给感知器（分类函数），并调整感知器，例如被误分类的点希望直线更加靠近它们。——这就是计算机在神经网络算法中，找寻最优感知器的原理。</p><h2 id="感知器算法">感知器算法</h2><p>感知器步骤如下所示。对于坐标轴为 <span class="math inline">\((p,q)\)</span> 的点，标签 y，以及等式 <span class="math inline">\(\hat{y} = step(w_1x_1 + w_2x_2 + b)\)</span> 给出的预测（其中 <span class="math inline">\(\alpha\)</span> 为学习率）：</p><ul><li>如果点分类正确，则什么也不做。</li><li>如果点分类为正，但是标签为负，则分别减去 <span class="math inline">\(\alpha p\)</span>, <span class="math inline">\(\alpha q\)</span>, 和 <span class="math inline">\(\alpha\)</span> 至 <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, 和 <span class="math inline">\(b\)</span>。</li><li>如果点分类为负，但是标签为正，则分别将 <span class="math inline">\(\alpha p\)</span>, <span class="math inline">\(\alpha q\)</span>, 和 <span class="math inline">\(\alpha\)</span> 加到 <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, 和 <span class="math inline">\(b\)</span> 上。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perceptronStep</span><span class="params">(X, y, W, b, learn_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        y_hat = prediction(X[i], W, b)</span><br><span class="line">        <span class="keyword">if</span> y[i] - y_hat == <span class="number">1</span>:</span><br><span class="line">            W[<span class="number">0</span>] += learn_rate * X[i][<span class="number">0</span>]</span><br><span class="line">            W[<span class="number">1</span>] += learn_rate * X[i][<span class="number">1</span>]</span><br><span class="line">            b += learn_rate</span><br><span class="line">        <span class="keyword">elif</span> y[i] - y_hat == <span class="number">-1</span>:</span><br><span class="line">            W[<span class="number">0</span>] -= learn_rate * X[i][<span class="number">0</span>]</span><br><span class="line">            W[<span class="number">1</span>] -= learn_rate * X[i][<span class="number">1</span>]</span><br><span class="line">            b -= learn_rate</span><br><span class="line">    <span class="keyword">return</span> W, b</span><br></pre></td></tr></table></figure><h1 id="误差函数">误差函数</h1><p>刚刚的感知器算法实现告诉我们，获取正确分类的方式，就是通过每一个错误分类的点，评估错误点位置与我们期望位置之间的差异，来慢慢的修正我们分类函数。</p><p>因为误差暗示了如何进行正确的分类，因此误差的定义就变得尤为重要，这也被称为<strong>误差函数</strong>，误差函数可以告诉我们当前与正确答案之间的差别有多大。</p><h2 id="误差函数与梯度下降">误差函数与梯度下降</h2><p>误差函数提供给我们的预测值与实际值之间的差异，但是这个差异如何指导我们权重的更新呢？我们的目标是找到<strong>最小</strong>的误差函数值来找到与实际值误差最小的预测值。</p><p>假设一维问题是一条直线，那么二维问题就是一个平面，而三维问题就是一个曲面。曲面可以理解为有山峰也有低谷的地面，误差最小的地方就是低谷处，我们希望计算机找到的就是这个低谷的值。为了找到这个低谷，学者们发明了<strong>梯度下降</strong>。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505152716.png"></p><p>为了进行梯度下降，我们的误差函数不能是离散的，而必须是连续的。误差之巅的高度是连续函数，因为位置上的轻微扰动会导致高度发生变化，实际上误差函数必须是<strong>可微分</strong>的。</p><h2 id="sigmoid-函数">Sigmoid 函数</h2><p>对于优化而言，连续型误差函数比离散型函数更好。为此，我们需要从离散型预测变成连续型预测，方法就是使用 Sigmoid 函数取代阶跃函数作为激活函数。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505155839.png"></p><p>Sigmoid 函数是一个在生物学中常见的 S 型函数，也称为 S 型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid 函数常被用作神经网络的阈值函数，将变量映射到 0,1 之间。</p><p>Sigmoid 函数由下列公式定义： <span class="math display">\[ \sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } } \]</span> 其对 <span class="math inline">\(x\)</span> 的导数可以用自身表示： <span class="math display">\[ \sigma ^ { \prime } ( x ) = \frac { e ^ { - x } } { \left( 1 + e ^ { - x } \right) ^ { 2 } } = \sigma ( x ) ( 1 - \sigma ( x ) ) \]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure><h2 id="softmax-函数">Softmax 函数</h2><p>Softmax 函数和 S 型函数是对等的，但是问题具有 3 个或更多个类别。</p><p>Softmax 公式： <span class="math display">\[ S _ { i } = \frac { e ^ { i } } { \sum _ { j } e ^ { j } } \]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(L)</span>:</span></span><br><span class="line">    expL = np.exp(L)</span><br><span class="line">    <span class="keyword">return</span> np.divide (expL, expL.sum())</span><br></pre></td></tr></table></figure><h2 id="one-hot-encoding">One-hot Encoding</h2><p>计算机在表示多结果的分类时，使用 One-Hot 编码是比较常见的处理方式。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190505163535.png"></p><h2 id="最大似然法">最大似然法</h2><p>最佳模型更有可能是对于实际在我们身上发生情况所对应的<strong>概率更大的模型</strong>，无论实际情况是被录取还是被拒，该方法叫做<strong>最大似然法（Maximum Likelihood）</strong>。</p><p>我们要做的是选出实际情况对应概率最大的模型，也就是说，通过<strong>最大化概率</strong>，我们可以选出最优的模型。</p><p>最大化概率是否就等价于最小化误差函数？也许的确是这样的。</p><h2 id="交叉熵">交叉熵</h2><p>概率和误差函数之间肯定有一定的联系，这种联系叫做<strong>交叉熵</strong>。这个概念在很多领域都非常流行，包括机器学习领域。</p><p>交叉熵具有以下特性：如果有一系列的事件及其对应的发生概率，如果事件发生的可能性大，则交叉熵较小；如果可能性小，那么交叉熵就会很大。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506093541.png"></p><p>交叉熵公式： <span class="math display">\[ CrossEntropy = - \sum _ { i = 1 } ^ { m } y_i \ln (p_i) + ( 1 - y_i ) \ln ( 1 - p_i ) \]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(Y, P)</span>:</span></span><br><span class="line">    Y = np.float_(Y)</span><br><span class="line">    P = np.float_(P)</span><br><span class="line">    <span class="keyword">return</span> -np.sum(Y * np.log(P) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - P))</span><br></pre></td></tr></table></figure><p>多类别交叉熵公式： <span class="math display">\[ CrossEntropy = - \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { m } y _ { i j } \ln \left( p _ { i j } \right) \]</span></p><h1 id="梯度下降">梯度下降</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506104821.png"></p><p>上图中的梯度 <span class="math inline">\(\nabla E\)</span> 实际告诉了我们误差函数增长最快的方向，因此，如果沿着该梯度的反方向，将得到误差函数降低最快的方向。 <span class="math display">\[ y = \sigma ( W x + b ) ← \text{Bad} \]</span></p><p><span class="math display">\[ y = \sigma \left( w _ { 1 } x _ { 1 } + \ldots + w _ { n } x _ { n } + b \right) \]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_formula</span><span class="params">(features, weights, bias)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(np.dot(features, weights) + bias)</span><br></pre></td></tr></table></figure><p><span class="math display">\[ \nabla E = \left( \partial E / \partial w _ { 1 } , \ldots , \partial E / \partial w _ { n } , \partial E / \partial b \right) \]</span></p><p>误差函数的梯度就是误差函数相对权重和偏差的偏导数所组成的矢量，我们按照以下方式更新权重和偏差（<span class="math inline">\(\alpha\)</span> 为学习率，可取为 0.1）： <span class="math display">\[ w _ { i } ^ { \prime } ← w _ { i } - \alpha \frac {\partial E} {\partial w _ { i }} \]</span></p><p><span class="math display">\[ b ^ { \prime } ← b - \alpha \frac {\partial E} {\partial b} \]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_weights</span><span class="params">(x, y, weights, bias, learnrate)</span>:</span></span><br><span class="line">    output = output_formula(x, weights, bias)</span><br><span class="line">    d_error = y - output</span><br><span class="line">    weights += learnrate * d_error * x</span><br><span class="line">    bias += learnrate * d_error</span><br><span class="line">    <span class="keyword">return</span> weights, bias</span><br></pre></td></tr></table></figure><p><span class="math display">\[ y = \sigma \left( W ^ { \prime } x + b ^ { \prime } \right) ← \text{Better} \]</span></p><h2 id="梯度计算">梯度计算</h2><p>Sigmoid 函数导数的计算：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506115945.png"></p><p>现在，如果有 <span class="math inline">\(m\)</span> 个样本点，标为 <span class="math inline">\(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\)</span>, 误差公式是： <span class="math display">\[ E = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( y ^ { ( i ) } \ln \left( \hat{y}^{(i)} \right) + \left( 1 - y ^ { ( i ) } \right) \ln \left( 1 - \hat{y}^{(i)} \right) \right) \]</span> 预测是 <span class="math inline">\(\hat{y}^{(i)} = \sigma(Wx^{(i)} + b)\)</span>，我们的目标是计算 <span class="math inline">\(E\)</span>, 在单个样本点 <span class="math inline">\(x\)</span> 时的梯度（偏导数），其中 <span class="math inline">\(x\)</span> 包含 n 个特征，即<span class="math inline">\(x = \left( x _ { 1 } , \ldots , x _ { n } \right)\)</span>. <span class="math display">\[ \nabla E = \left( \frac { \partial } { \partial w _ { 1 } } E , \cdots , \frac { \partial } { \partial w _ { n } } E , \frac { \partial } { \partial b } E \right) \]</span> 为此，首先我们要计算<span class="math inline">\(\frac { \partial } { \partial w _ { j } } \hat { y }\)</span>. 而 <span class="math inline">\(\hat { y } = \sigma ( W x + b )\)</span>，因此：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506123529.png"></p><p>最后一个等式是因为和中的唯一非常量项相对于 <span class="math inline">\(w_j\)</span> 正好是 <span class="math inline">\(w_j x_j\)</span>, 明显具有导数 <span class="math inline">\(x_j\)</span>. 现在可以计算 <span class="math inline">\(\frac { \partial } { \partial w _ { j } } E\)</span>：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506124100.png"></p><p>类似的计算将得出： <span class="math display">\[ \frac { \partial } { \partial b } E = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( y _ { i } - \hat { y } _ { i } \right) \]</span> 这个实际上告诉了我们很重要的规则。对于具有坐标 <span class="math inline">\((x_1, \ldots, x_n)\)</span>, 的点，标签 <span class="math inline">\(y\)</span>, 预测 <span class="math inline">\(\hat{y}\)</span>, 该点的误差函数梯度是 <span class="math inline">\(\left(-(y - \hat{y})x_1, \cdots, -(y - \hat{y})x_n, -(y - \hat{y}) \right)\)</span>.</p><p>总之 <span class="math inline">\(\nabla E ( W , b ) = - ( y - \hat { y } ) \left( x _ { 1 } , \ldots , x _ { n } , 1 \right)\)</span>.</p><p>如果思考下，会发现很神奇。<strong>梯度实际上是标量乘以点的坐标！</strong>什么是标量？也就是标签和预测之间的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。</p><p>请记下：<strong>小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。</strong></p><h2 id="感知器和梯度下降的区别">感知器和梯度下降的区别</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506160811.png"></p><p>左右几乎是完全相同的，唯一的区别是，在左侧 <span class="math inline">\(\hat{y}\)</span> 可以是 0 到 1 之间的任何值，但是在右侧 <span class="math inline">\(\hat{y}\)</span> 只能是值 0 或 1。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506161953.png"></p><p>对于分类正确的点感知器算法什么都不会做，但是梯度下降算法会修改权重使得点更加远离直线。</p><h1 id="神经网络结构">神经网络结构</h1><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162439.png"></p><p>为了处理非线性数据，需要创建非线性模型，我们可以把两个线性模型组合成一个非线性模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506162731.png"></p><p>现在的模型是两个之前的模型乘以权重，再加上某个偏差的线性组合。这就是构建神经网络的核心：对现有的线性模型进行线性组合，得到更复杂的新模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163008.png"></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163033.png"></p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163051.png"></p><p>我们可以通过感知器的组合来构建神经网络，其中左侧的权重告诉我们线性模型具有的方程，而右侧的权重告诉我们两个模型的线性组合是多少，以便获得右侧的曲线非线性模型。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163649.png"></p><p>每当你看到左侧的神经网络，思考下该神经网络定义的非线性界线是什么。</p><h2 id="多层级">多层级</h2><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506163918.png"></p><p>神经网络通常可以划分为输入层、隐藏层和输出层。除此之外我们可以执行以下操作：</p><ul><li>向输入、隐藏和输出层添加更多节点。</li><li>添加更多层级。</li></ul><p>更多隐藏层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164131.png"></p><p>这让我们得到输出层中的三角形边界！</p><p>更多输入层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506164240.png"></p><p>通常，如果输入层里有 n 个节点，那么处理的就是 n 维空间的数据。</p><p>更多输出层节点的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190506165731.png"></p><p>这只是表明我们有更多的输出，这就是具有多种类别的分类模型。</p><p>更多隐藏层的例子：</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507012253.png"></p><p>此时就形成了深度神经网络，现在这些线性模型相结合，形成非线性模型，这些非线性模型进一步再结合，形成更多的非线性模型。通常，我们可以这么操作很多次，形成具有大量隐藏层的复杂模型，这就是神经网络的神奇所在。</p><p>神经网络使用高度非线性化的边界拆分整个 n 维空间。</p><h2 id="前向反馈">前向反馈</h2><p>前向反馈是神经网络用来将输入变成输出的流程。</p><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507104424.png"></p><p>对于含有两个隐藏层的神经网络来说公式如下： <span class="math display">\[ \hat { y } = \sigma \circ W ^ { ( 3 ) } \circ \sigma \circ W ^ { ( 2 ) } \circ \sigma \circ W ^ { ( 1 ) } ( x ) \]</span></p><h2 id="反向传播">反向传播</h2><p>反向传播的流程：</p><ul><li>进行前向反馈运算。</li><li>将模型的输出与期望的输出进行比较。</li><li>计算误差。</li><li>向后运行前向反馈运算（反向传播），将误差分散到每个权重上。</li><li>更新权重，并获得更好的模型。</li><li>继续此流程，直到获得很好的模型。</li></ul><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507105250.png"></p><h3 id="链式法则">链式法则</h3><p><img data-src="https://raw.githubusercontent.com/lsj210001/FigureBed/master/img/20190507122314.png"></p><p>简而言之，对复合函数求导，就是一系列导数的乘积。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a> <a href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/" rel="tag"># 感知机</a> <a href="/tags/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0/" rel="tag"># 误差函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag"># 梯度下降</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/05/05/NoSQL-%E5%92%8C-SQL-%E7%9A%84%E5%8E%86%E5%8F%B2/" rel="prev" title="NoSQL 和 SQL 的历史"><i class="fa fa-chevron-left"></i> NoSQL 和 SQL 的历史</a></div><div class="post-nav-item"><a href="/2019/05/14/CNN-in-Tensorflow/" rel="next" title="CNN in Tensorflow">CNN in Tensorflow <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络的分类问题"><span class="nav-number">1.1.</span> <span class="nav-text">神经网络的分类问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#三维乃至更高维数据的情况"><span class="nav-number">1.1.1.</span> <span class="nav-text">三维乃至更高维数据的情况</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#感知器"><span class="nav-number">2.</span> <span class="nav-text">感知器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#用感知器实现逻辑运算---xor异或"><span class="nav-number">2.1.</span> <span class="nav-text">用感知器实现逻辑运算 - XOR（“异或”）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感知器技巧---计算机如何学习分类"><span class="nav-number">2.2.</span> <span class="nav-text">感知器技巧 - 计算机如何“学习”分类？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感知器算法"><span class="nav-number">2.3.</span> <span class="nav-text">感知器算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#误差函数"><span class="nav-number">3.</span> <span class="nav-text">误差函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#误差函数与梯度下降"><span class="nav-number">3.1.</span> <span class="nav-text">误差函数与梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid-函数"><span class="nav-number">3.2.</span> <span class="nav-text">Sigmoid 函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-函数"><span class="nav-number">3.3.</span> <span class="nav-text">Softmax 函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#one-hot-encoding"><span class="nav-number">3.4.</span> <span class="nav-text">One-hot Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大似然法"><span class="nav-number">3.5.</span> <span class="nav-text">最大似然法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵"><span class="nav-number">3.6.</span> <span class="nav-text">交叉熵</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降"><span class="nav-number">4.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度计算"><span class="nav-number">4.1.</span> <span class="nav-text">梯度计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感知器和梯度下降的区别"><span class="nav-number">4.2.</span> <span class="nav-text">感知器和梯度下降的区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络结构"><span class="nav-number">5.</span> <span class="nav-text">神经网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#多层级"><span class="nav-number">5.1.</span> <span class="nav-text">多层级</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向反馈"><span class="nav-number">5.2.</span> <span class="nav-text">前向反馈</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">5.3.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#链式法则"><span class="nav-number">5.3.1.</span> <span class="nav-text">链式法则</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Morgan" src="/images/apple-touch-icon-next.png"><p class="site-author-name" itemprop="name">Morgan</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xzajIxMDAwMQ==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lsj210001"><i class="fa fa-fw fa-github"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="bWFpbHRvOmR0Ymxsc2pAZ21haWwuY29t" title="E-Mail → mailto:dtbllsj@gmail.com"><i class="fa fa-fw fa-envelope"></i></span> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXR0ZXIuaW0vdGhlbWUtbmV4dA==" title="Gitter → https:&#x2F;&#x2F;gitter.im&#x2F;theme-next"><i class="fa fa-fw fa-github-alt"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9yaW90LmltL2FwcC8jL3Jvb20vI05leFQ6bWF0cml4Lm9yZw==" title="Riot → https:&#x2F;&#x2F;riot.im&#x2F;app&#x2F;#&#x2F;room&#x2F;#NexT:matrix.org"><i class="fa fa-fw fa-bullhorn"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL2pvaW5jaGF0L0dVTkhYQS12WmtnU011aW1MMVZtTXc=" title="Telegram [Chat] → https:&#x2F;&#x2F;t.me&#x2F;joinchat&#x2F;GUNHXA-vZkgSMuimL1VmMw"><i class="fa fa-fw fa-telegram"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly90Lm1lL3RoZW1lX25leHQ=" title="Telegram [News] → https:&#x2F;&#x2F;t.me&#x2F;theme_next"><i class="fa fa-fw fa-paper-plane"></i></span></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span></div><div class="cc-license motion-element" itemprop="sponsor"><span class="exturl cc-opacity" title="Deploy with Netlify → https://www.netlify.com" data-url="aHR0cHM6Ly93d3cubmV0bGlmeS5jb20="><img width="80" src="https://www.netlify.com/img/global/badges/netlify-dark.svg" alt="Netlify"></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2014 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">NexT</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">20k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">18 分钟</span></div><div class="footer-custom">Theme source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvaGV4by10aGVtZS1uZXh0">here</span><br>Website source code <span class="exturl theme-link" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RoZW1lLW5leHQvdGhlbWUtbmV4dC5vcmc=">here</span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>function loadCount(){var t=document,d=t.createElement("script");d.src="https://theme-next.disqus.com/count.js",d.id="dsq-count-scr",(t.head||t.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://lsj210001.github.io/2019/05/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/";
    this.page.identifier = "2019/05/05/神经网络简介/";
    this.page.title = "神经网络简介";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://theme-next.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html><!-- rebuild by neat -->